# ç¬¬4ç« ï¼šAIå¿…å¤‡æ•°å­¦åŸºç¡€

## ğŸ“ æœ¬ç« ç›®æ ‡
- æŒæ¡çº¿æ€§ä»£æ•°æ ¸å¿ƒæ¦‚å¿µ
- ç†è§£æ¦‚ç‡è®ºä¸ç»Ÿè®¡åŸºç¡€
- å­¦ä¹ å¾®ç§¯åˆ†åœ¨AIä¸­çš„åº”ç”¨
- æŒæ¡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•

## 4.1 çº¿æ€§ä»£æ•°

### 4.1.1 å‘é‡å’ŒçŸ©é˜µ

```python
import numpy as np

# å‘é‡è¿ç®—
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

print("å‘é‡åŠ æ³•:", v1 + v2)
print("ç‚¹ç§¯:", np.dot(v1, v2))
print("å‘é‡èŒƒæ•°:", np.linalg.norm(v1))

# çŸ©é˜µè¿ç®—
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

print("çŸ©é˜µä¹˜æ³•:\n", np.matmul(A, B))
print("çŸ©é˜µè½¬ç½®:\n", A.T)
print("çŸ©é˜µçš„é€†:\n", np.linalg.inv(A))
print("ç‰¹å¾å€¼:", np.linalg.eigvals(A))
```

### 4.1.2 çº¿æ€§å˜æ¢

```python
# æ—‹è½¬çŸ©é˜µ
theta = np.pi / 4  # 45åº¦
rotation_matrix = np.array([
    [np.cos(theta), -np.sin(theta)],
    [np.sin(theta), np.cos(theta)]
])

# åº”ç”¨å˜æ¢
point = np.array([1, 0])
rotated = rotation_matrix @ point
print("æ—‹è½¬åçš„ç‚¹:", rotated)
```

## 4.2 æ¦‚ç‡è®ºä¸ç»Ÿè®¡

### 4.2.1 æ¦‚ç‡åˆ†å¸ƒ

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# æ­£æ€åˆ†å¸ƒ
mu, sigma = 0, 1
x = np.linspace(-4, 4, 100)
y = stats.norm.pdf(x, mu, sigma)

plt.plot(x, y, label='æ­£æ€åˆ†å¸ƒ N(0,1)')
plt.xlabel('x')
plt.ylabel('æ¦‚ç‡å¯†åº¦')
plt.title('æ­£æ€åˆ†å¸ƒ')
plt.legend()
plt.grid(True)
plt.show()

# è´å¶æ–¯å®šç†åº”ç”¨
# P(A|B) = P(B|A) * P(A) / P(B)
def bayes_theorem(p_b_given_a, p_a, p_b):
    return (p_b_given_a * p_a) / p_b

# ç¤ºä¾‹ï¼šåŒ»ç–—è¯Šæ–­
p_positive_given_disease = 0.99  # æœ‰ç—…æµ‹å‡ºé˜³æ€§çš„æ¦‚ç‡
p_disease = 0.01  # æ‚£ç—…ç‡
p_positive = 0.05  # æµ‹å‡ºé˜³æ€§çš„æ¦‚ç‡

p_disease_given_positive = bayes_theorem(
    p_positive_given_disease, p_disease, p_positive
)
print(f"æµ‹å‡ºé˜³æ€§æ—¶çœŸæ‚£ç—…çš„æ¦‚ç‡: {p_disease_given_positive:.2%}")
```

### 4.2.2 æœŸæœ›ä¸æ–¹å·®

```python
# è®¡ç®—æœŸæœ›å’Œæ–¹å·®
data = np.random.randn(1000)

mean = np.mean(data)
variance = np.var(data)
std = np.std(data)

print(f"æœŸæœ›(å‡å€¼): {mean:.4f}")
print(f"æ–¹å·®: {variance:.4f}")
print(f"æ ‡å‡†å·®: {std:.4f}")

# åæ–¹å·®çŸ©é˜µ
X = np.random.randn(100, 3)
cov_matrix = np.cov(X.T)
print("åæ–¹å·®çŸ©é˜µ:\n", cov_matrix)
```

## 4.3 å¾®ç§¯åˆ†

### 4.3.1 å¯¼æ•°ä¸æ¢¯åº¦

```python
# æ•°å€¼æ±‚å¯¼
def numerical_gradient(f, x, h=1e-4):
    grad = np.zeros_like(x)
    for i in range(x.size):
        tmp_val = x[i]
        x[i] = tmp_val + h
        fxh1 = f(x)
        
        x[i] = tmp_val - h
        fxh2 = f(x)
        
        grad[i] = (fxh1 - fxh2) / (2*h)
        x[i] = tmp_val
    return grad

# ç¤ºä¾‹å‡½æ•° f(x,y) = x^2 + y^2
def f(x):
    return np.sum(x**2)

x = np.array([3.0, 4.0])
gradient = numerical_gradient(f, x)
print("æ¢¯åº¦:", gradient)  # [6.0, 8.0]
```

### 4.3.2 æ¢¯åº¦ä¸‹é™

```python
class GradientDescent:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.history = []
    
    def optimize(self, f, init_x, num_steps=100):
        x = init_x.copy()
        
        for i in range(num_steps):
            grad = numerical_gradient(f, x)
            x -= self.lr * grad
            self.history.append((x.copy(), f(x)))
        
        return x
    
    def plot_history(self):
        values = [v for _, v in self.history]
        plt.plot(values)
        plt.xlabel('è¿­ä»£æ¬¡æ•°')
        plt.ylabel('å‡½æ•°å€¼')
        plt.title('ä¼˜åŒ–è¿‡ç¨‹')
        plt.grid(True)
        plt.show()

# ä½¿ç”¨æ¢¯åº¦ä¸‹é™
optimizer = GradientDescent(lr=0.1)
init_x = np.array([5.0, 5.0])
optimal_x = optimizer.optimize(f, init_x)
print("æœ€ä¼˜è§£:", optimal_x)
optimizer.plot_history()
```

## 4.4 å®æˆ˜ï¼šçº¿æ€§å›å½’æ•°å­¦æ¨å¯¼

```python
class LinearRegressionMath:
    """ä»æ•°å­¦è§’åº¦ç†è§£çº¿æ€§å›å½’"""
    
    def __init__(self):
        self.w = None
        self.b = None
    
    def fit_normal_equation(self, X, y):
        """æ­£è§„æ–¹ç¨‹æ³• - è§£æè§£"""
        # æ·»åŠ åç½®åˆ—
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        
        # Î¸ = (X^T X)^-1 X^T y
        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y
        
        self.b = theta[0]
        self.w = theta[1:]
        
        return self
    
    def fit_gradient_descent(self, X, y, lr=0.01, epochs=1000):
        """æ¢¯åº¦ä¸‹é™æ³•"""
        m, n = X.shape
        self.w = np.zeros(n)
        self.b = 0
        
        for _ in range(epochs):
            # é¢„æµ‹
            y_pred = X @ self.w + self.b
            
            # è®¡ç®—æ¢¯åº¦
            dw = (1/m) * X.T @ (y_pred - y)
            db = (1/m) * np.sum(y_pred - y)
            
            # æ›´æ–°å‚æ•°
            self.w -= lr * dw
            self.b -= lr * db
        
        return self
    
    def predict(self, X):
        return X @ self.w + self.b

# ç¤ºä¾‹æ•°æ®
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X.squeeze() + np.random.randn(100)

# æ­£è§„æ–¹ç¨‹æ³•
model1 = LinearRegressionMath()
model1.fit_normal_equation(X, y)
print("æ­£è§„æ–¹ç¨‹ - w:", model1.w, "b:", model1.b)

# æ¢¯åº¦ä¸‹é™æ³•
model2 = LinearRegressionMath()
model2.fit_gradient_descent(X, y, lr=0.1, epochs=1000)
print("æ¢¯åº¦ä¸‹é™ - w:", model2.w, "b:", model2.b)
```

## 4.5 ä¿¡æ¯è®ºåŸºç¡€

### 4.5.1 ç†µä¸äº¤å‰ç†µ

```python
def entropy(p):
    """è®¡ç®—ç†µ"""
    return -np.sum(p * np.log2(p + 1e-10))

def cross_entropy(p, q):
    """äº¤å‰ç†µ"""
    return -np.sum(p * np.log2(q + 1e-10))

def kl_divergence(p, q):
    """KLæ•£åº¦"""
    return np.sum(p * np.log2((p + 1e-10) / (q + 1e-10)))

# ç¤ºä¾‹
p = np.array([0.5, 0.3, 0.2])
q = np.array([0.4, 0.4, 0.2])

print(f"ç†µ H(P): {entropy(p):.4f}")
print(f"äº¤å‰ç†µ H(P,Q): {cross_entropy(p, q):.4f}")
print(f"KLæ•£åº¦ D(P||Q): {kl_divergence(p, q):.4f}")
```

## 4.6 ä¼˜åŒ–ç®—æ³•å¯¹æ¯”

```python
class Optimizers:
    """å„ç§ä¼˜åŒ–ç®—æ³•å®ç°"""
    
    @staticmethod
    def sgd(params, grads, lr=0.01):
        """éšæœºæ¢¯åº¦ä¸‹é™"""
        for param, grad in zip(params, grads):
            param -= lr * grad
    
    @staticmethod
    def momentum(params, grads, v, lr=0.01, momentum=0.9):
        """åŠ¨é‡æ³•"""
        for param, grad, vel in zip(params, grads, v):
            vel[:] = momentum * vel - lr * grad
            param += vel
    
    @staticmethod
    def adam(params, grads, m, v, t, lr=0.001, beta1=0.9, beta2=0.999):
        """Adamä¼˜åŒ–å™¨"""
        eps = 1e-8
        t += 1
        
        for param, grad, mi, vi in zip(params, grads, m, v):
            mi[:] = beta1 * mi + (1 - beta1) * grad
            vi[:] = beta2 * vi + (1 - beta2) * (grad ** 2)
            
            m_hat = mi / (1 - beta1 ** t)
            v_hat = vi / (1 - beta2 ** t)
            
            param -= lr * m_hat / (np.sqrt(v_hat) + eps)
        
        return t
```

## ğŸ“š æœ¬ç« å°ç»“
- âœ… çº¿æ€§ä»£æ•°ï¼šå‘é‡ã€çŸ©é˜µè¿ç®—
- âœ… æ¦‚ç‡ç»Ÿè®¡ï¼šåˆ†å¸ƒã€è´å¶æ–¯å®šç†
- âœ… å¾®ç§¯åˆ†ï¼šå¯¼æ•°ã€æ¢¯åº¦ä¸‹é™
- âœ… ä¿¡æ¯è®ºï¼šç†µã€äº¤å‰ç†µã€KLæ•£åº¦
- âœ… ä¼˜åŒ–ç®—æ³•ï¼šSGDã€Momentumã€Adam

## ğŸ¯ ç»ƒä¹ é¢˜
1. å®ç°çŸ©é˜µçš„ç‰¹å¾å€¼åˆ†è§£
2. ç”¨è´å¶æ–¯å®šç†è§£å†³å®é™…é—®é¢˜
3. æ¯”è¾ƒä¸åŒä¼˜åŒ–ç®—æ³•çš„æ”¶æ•›é€Ÿåº¦
4. æ¨å¯¼é€»è¾‘å›å½’çš„æŸå¤±å‡½æ•°æ¢¯åº¦

[â¬…ï¸ ä¸Šä¸€ç« ](./03-Pythonç¼–ç¨‹åŸºç¡€.md) | [è¿”å›ç›®å½•](../README.md) | [ä¸‹ä¸€ç«  â¡ï¸](../02-è¿›é˜¶ç¯‡/05-æ·±åº¦å­¦ä¹ åŸºç¡€.md)
