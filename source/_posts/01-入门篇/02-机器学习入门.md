# ç¬¬2ç« ï¼šæœºå™¨å­¦ä¹ å…¥é—¨

## ğŸ“ æœ¬ç« ç›®æ ‡

- ç†è§£æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œåˆ†ç±»
- æŒæ¡ç›‘ç£å­¦ä¹ ã€éç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ çš„åŒºåˆ«
- å­¦ä¹ å¸¸è§çš„æœºå™¨å­¦ä¹ ç®—æ³•
- é€šè¿‡æˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹å®è·µæœºå™¨å­¦ä¹ æµç¨‹

---

## 2.1 ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ 

### å®šä¹‰

**æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰** æ˜¯ä¸€ç§è®©è®¡ç®—æœºç³»ç»Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›çš„æŠ€æœ¯ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚

**ä¼ ç»Ÿç¼–ç¨‹ vs æœºå™¨å­¦ä¹ **

```
ä¼ ç»Ÿç¼–ç¨‹:
è¾“å…¥æ•°æ® + ç¨‹åºè§„åˆ™ â†’ è¾“å‡ºç»“æœ

æœºå™¨å­¦ä¹ :
è¾“å…¥æ•°æ® + è¾“å‡ºç»“æœ â†’ å­¦ä¹ è§„åˆ™ï¼ˆæ¨¡å‹ï¼‰
```

### æ ¸å¿ƒæ€æƒ³

> "ä¸æ˜¯æ•™è®¡ç®—æœºå¦‚ä½•åšï¼Œè€Œæ˜¯è®©è®¡ç®—æœºä»ç»éªŒä¸­å­¦ä¹ å¦‚ä½•åš"

---

## 2.2 æœºå™¨å­¦ä¹ çš„ä¸‰å¤§ç±»å‹

### 2.2.1 ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰

**å®šä¹‰**ï¼šä»æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ï¼Œé¢„æµ‹æ–°æ•°æ®çš„æ ‡ç­¾ã€‚

**ç‰¹ç‚¹**ï¼š
- æœ‰æ˜ç¡®çš„è¾“å…¥å’Œè¾“å‡º
- éœ€è¦äººå·¥æ ‡æ³¨çš„æ•°æ®
- ç›®æ ‡æ˜¯å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„

**åº”ç”¨åœºæ™¯**ï¼š

| ä»»åŠ¡ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|---------|------|------|
| **åˆ†ç±»** | é¢„æµ‹ç¦»æ•£ç±»åˆ« | åƒåœ¾é‚®ä»¶è¯†åˆ«ã€å›¾åƒåˆ†ç±»ã€ç–¾ç—…è¯Šæ–­ |
| **å›å½’** | é¢„æµ‹è¿ç»­æ•°å€¼ | æˆ¿ä»·é¢„æµ‹ã€è‚¡ç¥¨ä»·æ ¼ã€é”€é‡é¢„æµ‹ |

**ç¤ºä¾‹ä»£ç ï¼šåˆ†ç±»**

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®
iris = load_iris()
X, y = iris.data, iris.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# è®­ç»ƒæ¨¡å‹
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f"å‡†ç¡®ç‡: {accuracy*100:.2f}%")
```

### 2.2.2 éç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰

**å®šä¹‰**ï¼šä»æœªæ ‡æ³¨çš„æ•°æ®ä¸­å‘ç°éšè—çš„æ¨¡å¼å’Œç»“æ„ã€‚

**ç‰¹ç‚¹**ï¼š
- æ²¡æœ‰æ˜ç¡®çš„æ ‡ç­¾
- è‡ªåŠ¨å‘ç°æ•°æ®çš„å†…åœ¨è§„å¾‹
- ç”¨äºæ¢ç´¢æ€§æ•°æ®åˆ†æ

**åº”ç”¨åœºæ™¯**ï¼š

| ä»»åŠ¡ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|---------|------|------|
| **èšç±»** | å°†ç›¸ä¼¼æ•°æ®åˆ†ç»„ | å®¢æˆ·åˆ†ç¾¤ã€å›¾åƒåˆ†å‰² |
| **é™ç»´** | å‡å°‘æ•°æ®ç»´åº¦ | æ•°æ®å¯è§†åŒ–ã€ç‰¹å¾æå– |
| **å¼‚å¸¸æ£€æµ‹** | å‘ç°å¼‚å¸¸æ•°æ®ç‚¹ | æ¬ºè¯ˆæ£€æµ‹ã€è®¾å¤‡æ•…éšœé¢„è­¦ |

**ç¤ºä¾‹ä»£ç ï¼šèšç±»**

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
np.random.seed(42)
X = np.concatenate([
    np.random.randn(100, 2) + [2, 2],
    np.random.randn(100, 2) + [-2, -2],
    np.random.randn(100, 2) + [2, -2]
])

# K-Meansèšç±»
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# å¯è§†åŒ–
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], 
           kmeans.cluster_centers_[:, 1],
           marker='X', s=200, c='red', label='ä¸­å¿ƒç‚¹')
plt.title('K-Means èšç±»ç»“æœ')
plt.legend()
plt.show()
```

### 2.2.3 å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰

**å®šä¹‰**ï¼šé€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œå­¦ä¹ å¦‚ä½•åšå‡ºæœ€ä¼˜å†³ç­–ã€‚

**ç‰¹ç‚¹**ï¼š
- é€šè¿‡è¯•é”™å­¦ä¹ 
- æœ‰å¥–åŠ±ä¿¡å·æŒ‡å¯¼
- ç›®æ ‡æ˜¯æœ€å¤§åŒ–é•¿æœŸå›æŠ¥

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š

```
æ™ºèƒ½ä½“ï¼ˆAgentï¼‰ â†â†’ ç¯å¢ƒï¼ˆEnvironmentï¼‰
        â†“
      çŠ¶æ€ï¼ˆStateï¼‰
        â†“
      åŠ¨ä½œï¼ˆActionï¼‰
        â†“
      å¥–åŠ±ï¼ˆRewardï¼‰
```

**åº”ç”¨åœºæ™¯**ï¼š
- æ¸¸æˆAIï¼ˆAlphaGoã€Dota 2ï¼‰
- æœºå™¨äººæ§åˆ¶
- è‡ªåŠ¨é©¾é©¶
- æ¨èç³»ç»Ÿä¼˜åŒ–

**ç¤ºä¾‹ï¼šç®€å•çš„Q-Learning**

```python
import numpy as np

class QLearningAgent:
    def __init__(self, n_states, n_actions, learning_rate=0.1, gamma=0.9):
        self.q_table = np.zeros((n_states, n_actions))
        self.lr = learning_rate
        self.gamma = gamma
    
    def choose_action(self, state, epsilon=0.1):
        # Îµ-è´ªå¿ƒç­–ç•¥
        if np.random.random() < epsilon:
            return np.random.randint(self.q_table.shape[1])
        else:
            return np.argmax(self.q_table[state])
    
    def learn(self, state, action, reward, next_state):
        # Q-Learningæ›´æ–°å…¬å¼
        predict = self.q_table[state, action]
        target = reward + self.gamma * np.max(self.q_table[next_state])
        self.q_table[state, action] += self.lr * (target - predict)

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆç®€åŒ–çš„è¿·å®«ç¯å¢ƒï¼‰
agent = QLearningAgent(n_states=16, n_actions=4)

# è®­ç»ƒå¾ªç¯
for episode in range(1000):
    state = 0  # èµ·å§‹çŠ¶æ€
    while state != 15:  # ç›´åˆ°åˆ°è¾¾ç»ˆç‚¹
        action = agent.choose_action(state)
        # next_state, reward = env.step(action)  # ç¯å¢ƒäº¤äº’
        # agent.learn(state, action, reward, next_state)
        # state = next_state
        pass
```

---

## 2.3 å¸¸è§æœºå™¨å­¦ä¹ ç®—æ³•

### 2.3.1 çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰

**åŸç†**ï¼šæ‰¾åˆ°æœ€ä½³æ‹Ÿåˆç›´çº¿ y = wx + b

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# ç¤ºä¾‹æ•°æ®
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])

# è®­ç»ƒæ¨¡å‹
model = LinearRegression()
model.fit(X, y)

# é¢„æµ‹
y_pred = model.predict([[6]])
print(f"é¢„æµ‹å€¼: {y_pred[0]:.2f}")
print(f"ç³»æ•°: {model.coef_[0]:.2f}, æˆªè·: {model.intercept_:.2f}")
```

### 2.3.2 é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰

**åŸç†**ï¼šç”¨äºäºŒåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºæ¦‚ç‡å€¼

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# ç”ŸæˆäºŒåˆ†ç±»æ•°æ®
X, y = make_classification(n_samples=100, n_features=2, 
                          n_redundant=0, random_state=42)

# è®­ç»ƒæ¨¡å‹
model = LogisticRegression()
model.fit(X, y)

# é¢„æµ‹æ¦‚ç‡
probabilities = model.predict_proba(X[:5])
print("å‰5ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡:")
print(probabilities)
```

### 2.3.3 å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰

**åŸç†**ï¼šé€šè¿‡æ ‘å½¢ç»“æ„è¿›è¡Œå†³ç­–

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# è®­ç»ƒå†³ç­–æ ‘
model = DecisionTreeClassifier(max_depth=3)
model.fit(X_train, y_train)

# å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(15, 10))
plot_tree(model, filled=True, feature_names=iris.feature_names,
         class_names=iris.target_names)
plt.show()
```

### 2.3.4 éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰

**åŸç†**ï¼šé›†æˆå¤šæ£µå†³ç­–æ ‘ï¼ŒæŠ•ç¥¨å†³å®šç»“æœ

```python
from sklearn.ensemble import RandomForestClassifier

# è®­ç»ƒéšæœºæ£®æ—
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ç‰¹å¾é‡è¦æ€§
importances = model.feature_importances_
for name, importance in zip(iris.feature_names, importances):
    print(f"{name}: {importance:.4f}")
```

### 2.3.5 æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰

**åŸç†**ï¼šå¯»æ‰¾æœ€ä¼˜åˆ†ç±»è¶…å¹³é¢

```python
from sklearn.svm import SVC

# è®­ç»ƒSVM
model = SVC(kernel='rbf', C=1.0)
model.fit(X_train, y_train)

# é¢„æµ‹
accuracy = model.score(X_test, y_test)
print(f"SVMå‡†ç¡®ç‡: {accuracy*100:.2f}%")
```

### 2.3.6 Kè¿‘é‚»ï¼ˆK-Nearest Neighborsï¼‰

**åŸç†**ï¼šæ ¹æ®æœ€è¿‘çš„Kä¸ªé‚»å±…æŠ•ç¥¨å†³å®šç±»åˆ«

```python
from sklearn.neighbors import KNeighborsClassifier

# è®­ç»ƒKNN
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

# é¢„æµ‹
predictions = model.predict(X_test)
```

---

## 2.4 æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹

### æ ‡å‡†æµç¨‹

```
1. é—®é¢˜å®šä¹‰
    â†“
2. æ•°æ®æ”¶é›†
    â†“
3. æ•°æ®æ¢ç´¢ä¸æ¸…æ´—
    â†“
4. ç‰¹å¾å·¥ç¨‹
    â†“
5. æ¨¡å‹é€‰æ‹©
    â†“
6. æ¨¡å‹è®­ç»ƒ
    â†“
7. æ¨¡å‹è¯„ä¼°
    â†“
8. æ¨¡å‹ä¼˜åŒ–
    â†“
9. æ¨¡å‹éƒ¨ç½²
    â†“
10. ç›‘æ§ä¸ç»´æŠ¤
```

---

## 2.5 å®æˆ˜æ¡ˆä¾‹ï¼šæˆ¿ä»·é¢„æµ‹ç³»ç»Ÿ

### æ¡ˆä¾‹èƒŒæ™¯

å¼€å‘ä¸€ä¸ªæˆ¿ä»·é¢„æµ‹æ¨¡å‹ï¼Œæ ¹æ®æˆ¿å±‹çš„ç‰¹å¾ï¼ˆé¢ç§¯ã€æˆ¿é—´æ•°ã€åœ°ç†ä½ç½®ç­‰ï¼‰é¢„æµ‹æˆ¿ä»·ã€‚

### å®Œæ•´ä»£ç å®ç°

```python
# æˆ¿ä»·é¢„æµ‹å®Œæ•´é¡¹ç›®
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ====== 1. æ•°æ®åŠ è½½ ======
# ä½¿ç”¨æ³¢å£«é¡¿æˆ¿ä»·æ•°æ®é›†ï¼ˆç¤ºä¾‹ï¼‰
from sklearn.datasets import fetch_california_housing

data = fetch_california_housing()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['PRICE'] = data.target

print("æ•°æ®é›†å½¢çŠ¶:", df.shape)
print("\nå‰5è¡Œæ•°æ®:")
print(df.head())

# ====== 2. æ•°æ®æ¢ç´¢ ======
print("\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
print(df.describe())

print("\nç¼ºå¤±å€¼æ£€æŸ¥:")
print(df.isnull().sum())

# ç›¸å…³æ€§åˆ†æ
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾')
plt.tight_layout()
plt.savefig('correlation_heatmap.png')
plt.show()

# ====== 3. ç‰¹å¾å·¥ç¨‹ ======
# åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
X = df.drop('PRICE', axis=1)
y = df['PRICE']

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nè®­ç»ƒé›†å¤§å°: {X_train.shape}")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")

# ====== 4. æ¨¡å‹è®­ç»ƒä¸å¯¹æ¯” ======
models = {
    'çº¿æ€§å›å½’': LinearRegression(),
    'Ridgeå›å½’': Ridge(alpha=1.0),
    'Lassoå›å½’': Lasso(alpha=0.1),
    'éšæœºæ£®æ—': RandomForestRegressor(n_estimators=100, random_state=42),
    'æ¢¯åº¦æå‡': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

results = {}

for name, model in models.items():
    print(f"\n{'='*50}")
    print(f"è®­ç»ƒæ¨¡å‹: {name}")
    print('='*50)
    
    # è®­ç»ƒ
    if name in ['çº¿æ€§å›å½’', 'Ridgeå›å½’', 'Lassoå›å½’']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
    
    # è¯„ä¼°
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    results[name] = {
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2,
        'model': model
    }
    
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.4f}")
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.4f}")
    print(f"RÂ² åˆ†æ•°: {r2:.4f}")

# ====== 5. ç»“æœå¯è§†åŒ– ======
# æ¨¡å‹æ€§èƒ½å¯¹æ¯”
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

metrics = ['RMSE', 'MAE', 'R2']
for idx, metric in enumerate(metrics):
    values = [results[model][metric] for model in models.keys()]
    axes[idx].bar(models.keys(), values, color='skyblue')
    axes[idx].set_title(f'{metric} å¯¹æ¯”')
    axes[idx].set_xlabel('æ¨¡å‹')
    axes[idx].set_ylabel(metric)
    axes[idx].tick_params(axis='x', rotation=45)
    
    # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
    for i, v in enumerate(values):
        axes[idx].text(i, v, f'{v:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig('model_comparison.png')
plt.show()

# é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆRÂ²æœ€é«˜ï¼‰
best_model_name = max(results, key=lambda x: results[x]['R2'])
best_model = results[best_model_name]['model']

print(f"\n{'='*50}")
print(f"æœ€ä½³æ¨¡å‹: {best_model_name}")
print(f"RÂ² åˆ†æ•°: {results[best_model_name]['R2']:.4f}")
print('='*50)

# ====== 6. é¢„æµ‹ vs å®é™…å€¼å¯è§†åŒ– ======
if best_model_name in ['çº¿æ€§å›å½’', 'Ridgeå›å½’', 'Lassoå›å½’']:
    y_pred_best = best_model.predict(X_test_scaled)
else:
    y_pred_best = best_model.predict(X_test)

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_best, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
         'r--', lw=2, label='å®Œç¾é¢„æµ‹çº¿')
plt.xlabel('å®é™…æˆ¿ä»·')
plt.ylabel('é¢„æµ‹æˆ¿ä»·')
plt.title(f'{best_model_name} - é¢„æµ‹ vs å®é™…')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('prediction_vs_actual.png')
plt.show()

# ====== 7. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆé’ˆå¯¹éšæœºæ£®æ—ï¼‰ ======
if best_model_name == 'éšæœºæ£®æ—':
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance, x='importance', y='feature')
    plt.title('ç‰¹å¾é‡è¦æ€§æ’å')
    plt.xlabel('é‡è¦æ€§')
    plt.tight_layout()
    plt.savefig('feature_importance.png')
    plt.show()
    
    print("\nç‰¹å¾é‡è¦æ€§:")
    print(feature_importance)

# ====== 8. å®ç”¨é¢„æµ‹å‡½æ•° ======
class HousePricePredictor:
    def __init__(self, model, scaler, feature_names, is_scaled=False):
        self.model = model
        self.scaler = scaler
        self.feature_names = feature_names
        self.is_scaled = is_scaled
    
    def predict_single(self, house_features):
        """
        é¢„æµ‹å•ä¸ªæˆ¿å±‹ä»·æ ¼
        
        å‚æ•°:
            house_features: dict, æˆ¿å±‹ç‰¹å¾å­—å…¸
        è¿”å›:
            é¢„æµ‹ä»·æ ¼
        """
        # è½¬æ¢ä¸ºDataFrame
        df_input = pd.DataFrame([house_features])
        
        # ç¡®ä¿åˆ—é¡ºåºæ­£ç¡®
        df_input = df_input[self.feature_names]
        
        # æ ‡å‡†åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.is_scaled:
            df_input = self.scaler.transform(df_input)
        
        # é¢„æµ‹
        price = self.model.predict(df_input)[0]
        
        return price
    
    def batch_predict(self, houses_list):
        """æ‰¹é‡é¢„æµ‹"""
        predictions = []
        for house in houses_list:
            price = self.predict_single(house)
            predictions.append(price)
        return predictions

# åˆ›å»ºé¢„æµ‹å™¨
predictor = HousePricePredictor(
    model=best_model,
    scaler=scaler,
    feature_names=X.columns,
    is_scaled=(best_model_name in ['çº¿æ€§å›å½’', 'Ridgeå›å½’', 'Lassoå›å½’'])
)

# ç¤ºä¾‹é¢„æµ‹
example_house = {
    'MedInc': 3.5,
    'HouseAge': 15,
    'AveRooms': 6,
    'AveBedrms': 1.2,
    'Population': 1000,
    'AveOccup': 3,
    'Latitude': 37.5,
    'Longitude': -122.0
}

predicted_price = predictor.predict_single(example_house)
print(f"\nç¤ºä¾‹æˆ¿å±‹é¢„æµ‹ä»·æ ¼: ${predicted_price*100000:.2f}")

# ====== 9. æ¨¡å‹ä¿å­˜ ======
import joblib

joblib.dump(best_model, 'house_price_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
print("\næ¨¡å‹å·²ä¿å­˜!")

# ====== 10. æ¨¡å‹åŠ è½½ä¸ä½¿ç”¨ ======
loaded_model = joblib.load('house_price_model.pkl')
loaded_scaler = joblib.load('scaler.pkl')

print("\næ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨!")
```

### è¾“å‡ºç¤ºä¾‹

```
æ•°æ®é›†å½¢çŠ¶: (20640, 9)

æ¨¡å‹æ€§èƒ½å¯¹æ¯”:
==================================================
çº¿æ€§å›å½’ - RMSE: 0.7344, RÂ²: 0.5757
Ridgeå›å½’ - RMSE: 0.7344, RÂ²: 0.5757
Lassoå›å½’ - RMSE: 0.7376, RÂ²: 0.5720
éšæœºæ£®æ— - RMSE: 0.5234, RÂ²: 0.8021  â­ æœ€ä½³
æ¢¯åº¦æå‡ - RMSE: 0.5567, RÂ²: 0.7764
==================================================

æœ€ä½³æ¨¡å‹: éšæœºæ£®æ—
RÂ² åˆ†æ•°: 0.8021

ç¤ºä¾‹æˆ¿å±‹é¢„æµ‹ä»·æ ¼: $234,500.00
```

---

## 2.6 æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

### å›å½’é—®é¢˜æŒ‡æ ‡

| æŒ‡æ ‡ | å…¬å¼ | å«ä¹‰ | å–å€¼èŒƒå›´ |
|------|------|------|----------|
| **MAE** | $\frac{1}{n}\sum\|y_i - \hat{y}_i\|$ | å¹³å‡ç»å¯¹è¯¯å·® | [0, +âˆ) |
| **MSE** | $\frac{1}{n}\sum(y_i - \hat{y}_i)^2$ | å‡æ–¹è¯¯å·® | [0, +âˆ) |
| **RMSE** | $\sqrt{MSE}$ | å‡æ–¹æ ¹è¯¯å·® | [0, +âˆ) |
| **RÂ²** | $1 - \frac{SS_{res}}{SS_{tot}}$ | å†³å®šç³»æ•° | (-âˆ, 1] |

### åˆ†ç±»é—®é¢˜æŒ‡æ ‡

| æŒ‡æ ‡ | è®¡ç®—æ–¹å¼ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|
| **å‡†ç¡®ç‡** | æ­£ç¡®é¢„æµ‹æ•° / æ€»æ•° | ç±»åˆ«å‡è¡¡æ—¶ |
| **ç²¾ç¡®ç‡** | TP / (TP + FP) | å…³æ³¨è¯¯æŠ¥ |
| **å¬å›ç‡** | TP / (TP + FN) | å…³æ³¨æ¼æŠ¥ |
| **F1åˆ†æ•°** | 2 Ã— (ç²¾ç¡®ç‡ Ã— å¬å›ç‡) / (ç²¾ç¡®ç‡ + å¬å›ç‡) | ç»¼åˆè¯„ä¼° |

```python
from sklearn.metrics import classification_report, confusion_matrix

# åˆ†ç±»æŠ¥å‘Š
print(classification_report(y_test, y_pred))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('æ··æ·†çŸ©é˜µ')
plt.show()
```

---

## 2.7 è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ

### æ¦‚å¿µ

```
æ¬ æ‹Ÿåˆ          åˆšåˆšå¥½          è¿‡æ‹Ÿåˆ
(Underfitting)  (Good Fit)     (Overfitting)
    
ç®€å•æ¨¡å‹        åˆé€‚æ¨¡å‹        å¤æ‚æ¨¡å‹
è®­ç»ƒè¯¯å·®å¤§      è®­ç»ƒè¯¯å·®å°      è®­ç»ƒè¯¯å·®æå°
æµ‹è¯•è¯¯å·®å¤§      æµ‹è¯•è¯¯å·®å°      æµ‹è¯•è¯¯å·®å¤§
```

### è§£å†³æ–¹æ³•

**æ¬ æ‹Ÿåˆ**ï¼š
- å¢åŠ æ¨¡å‹å¤æ‚åº¦
- å¢åŠ ç‰¹å¾
- å‡å°‘æ­£åˆ™åŒ–

**è¿‡æ‹Ÿåˆ**ï¼š
- è·å–æ›´å¤šæ•°æ®
- ç‰¹å¾é€‰æ‹©
- æ­£åˆ™åŒ–ï¼ˆL1ã€L2ï¼‰
- Early Stopping
- Dropoutï¼ˆæ·±åº¦å­¦ä¹ ï¼‰

```python
# ä½¿ç”¨äº¤å‰éªŒè¯æ£€æµ‹è¿‡æ‹Ÿåˆ
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"äº¤å‰éªŒè¯RÂ²åˆ†æ•°: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
```

---

## ğŸ“š æœ¬ç« å°ç»“

- æœºå™¨å­¦ä¹ åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€éç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ä¸‰å¤§ç±»
- ç›‘ç£å­¦ä¹ åŒ…æ‹¬åˆ†ç±»å’Œå›å½’ä»»åŠ¡
- å¸¸è§ç®—æ³•ï¼šçº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€SVMã€KNN
- å®Œæ•´çš„MLæµç¨‹ï¼šæ•°æ®â†’ç‰¹å¾â†’æ¨¡å‹â†’è¯„ä¼°â†’éƒ¨ç½²
- é€šè¿‡æˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹æŒæ¡äº†å®æˆ˜æµç¨‹
- éœ€è¦æ³¨æ„è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆé—®é¢˜

---

## ğŸ¯ ç»ƒä¹ é¢˜

1. **æ¦‚å¿µé¢˜**ï¼šè§£é‡Šç›‘ç£å­¦ä¹ å’Œéç›‘ç£å­¦ä¹ çš„åŒºåˆ«ï¼Œå„ä¸¾3ä¸ªåº”ç”¨ä¾‹å­
2. **å®è·µé¢˜**ï¼šè¿è¡Œæˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹ï¼Œå°è¯•æ·»åŠ æ–°ç‰¹å¾æå‡æ¨¡å‹æ€§èƒ½
3. **å¯¹æ¯”é¢˜**ï¼šåœ¨åŒä¸€æ•°æ®é›†ä¸Šå¯¹æ¯”è‡³å°‘3ç§ç®—æ³•ï¼Œåˆ†æå„è‡ªä¼˜ç¼ºç‚¹
4. **æ€è€ƒé¢˜**ï¼šå¦‚ä½•åˆ¤æ–­æ¨¡å‹æ˜¯è¿‡æ‹Ÿåˆè¿˜æ˜¯æ¬ æ‹Ÿåˆï¼Ÿå¦‚ä½•è§£å†³ï¼Ÿ

---

[â¬…ï¸ ä¸Šä¸€ç« ](./01-AIåŸºç¡€æ¦‚å¿µ.md) | [è¿”å›ç›®å½•](../README.md) | [ä¸‹ä¸€ç« ï¼šPythonç¼–ç¨‹åŸºç¡€ â¡ï¸](./03-Pythonç¼–ç¨‹åŸºç¡€.md)
