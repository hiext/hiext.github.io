# 第2章：机器学习入门

## 📝 本章目标

- 理解机器学习的基本概念和分类
- 掌握监督学习、非监督学习、强化学习的区别
- 学习常见的机器学习算法
- 通过房价预测案例实践机器学习流程

---

## 2.1 什么是机器学习

### 定义

**机器学习（Machine Learning）** 是一种让计算机系统从数据中自动学习和改进的技术，无需明确编程。

**传统编程 vs 机器学习**

```
传统编程:
输入数据 + 程序规则 → 输出结果

机器学习:
输入数据 + 输出结果 → 学习规则（模型）
```

### 核心思想

> "不是教计算机如何做，而是让计算机从经验中学习如何做"

---

## 2.2 机器学习的三大类型

### 2.2.1 监督学习（Supervised Learning）

**定义**：从标注的训练数据中学习，预测新数据的标签。

**特点**：
- 有明确的输入和输出
- 需要人工标注的数据
- 目标是学习输入到输出的映射

**应用场景**：

| 任务类型 | 说明 | 示例 |
|---------|------|------|
| **分类** | 预测离散类别 | 垃圾邮件识别、图像分类、疾病诊断 |
| **回归** | 预测连续数值 | 房价预测、股票价格、销量预测 |

**示例代码：分类**

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# 加载数据
iris = load_iris()
X, y = iris.data, iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 训练模型
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 评估
accuracy = accuracy_score(y_test, y_pred)
print(f"准确率: {accuracy*100:.2f}%")
```

### 2.2.2 非监督学习（Unsupervised Learning）

**定义**：从未标注的数据中发现隐藏的模式和结构。

**特点**：
- 没有明确的标签
- 自动发现数据的内在规律
- 用于探索性数据分析

**应用场景**：

| 任务类型 | 说明 | 示例 |
|---------|------|------|
| **聚类** | 将相似数据分组 | 客户分群、图像分割 |
| **降维** | 减少数据维度 | 数据可视化、特征提取 |
| **异常检测** | 发现异常数据点 | 欺诈检测、设备故障预警 |

**示例代码：聚类**

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np

# 生成示例数据
np.random.seed(42)
X = np.concatenate([
    np.random.randn(100, 2) + [2, 2],
    np.random.randn(100, 2) + [-2, -2],
    np.random.randn(100, 2) + [2, -2]
])

# K-Means聚类
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# 可视化
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], 
           kmeans.cluster_centers_[:, 1],
           marker='X', s=200, c='red', label='中心点')
plt.title('K-Means 聚类结果')
plt.legend()
plt.show()
```

### 2.2.3 强化学习（Reinforcement Learning）

**定义**：通过与环境交互，学习如何做出最优决策。

**特点**：
- 通过试错学习
- 有奖励信号指导
- 目标是最大化长期回报

**核心概念**：

```
智能体（Agent） ←→ 环境（Environment）
        ↓
      状态（State）
        ↓
      动作（Action）
        ↓
      奖励（Reward）
```

**应用场景**：
- 游戏AI（AlphaGo、Dota 2）
- 机器人控制
- 自动驾驶
- 推荐系统优化

**示例：简单的Q-Learning**

```python
import numpy as np

class QLearningAgent:
    def __init__(self, n_states, n_actions, learning_rate=0.1, gamma=0.9):
        self.q_table = np.zeros((n_states, n_actions))
        self.lr = learning_rate
        self.gamma = gamma
    
    def choose_action(self, state, epsilon=0.1):
        # ε-贪心策略
        if np.random.random() < epsilon:
            return np.random.randint(self.q_table.shape[1])
        else:
            return np.argmax(self.q_table[state])
    
    def learn(self, state, action, reward, next_state):
        # Q-Learning更新公式
        predict = self.q_table[state, action]
        target = reward + self.gamma * np.max(self.q_table[next_state])
        self.q_table[state, action] += self.lr * (target - predict)

# 使用示例（简化的迷宫环境）
agent = QLearningAgent(n_states=16, n_actions=4)

# 训练循环
for episode in range(1000):
    state = 0  # 起始状态
    while state != 15:  # 直到到达终点
        action = agent.choose_action(state)
        # next_state, reward = env.step(action)  # 环境交互
        # agent.learn(state, action, reward, next_state)
        # state = next_state
        pass
```

---

## 2.3 常见机器学习算法

### 2.3.1 线性回归（Linear Regression）

**原理**：找到最佳拟合直线 y = wx + b

```python
from sklearn.linear_model import LinearRegression
import numpy as np

# 示例数据
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 5, 4, 5])

# 训练模型
model = LinearRegression()
model.fit(X, y)

# 预测
y_pred = model.predict([[6]])
print(f"预测值: {y_pred[0]:.2f}")
print(f"系数: {model.coef_[0]:.2f}, 截距: {model.intercept_:.2f}")
```

### 2.3.2 逻辑回归（Logistic Regression）

**原理**：用于二分类问题，输出概率值

```python
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# 生成二分类数据
X, y = make_classification(n_samples=100, n_features=2, 
                          n_redundant=0, random_state=42)

# 训练模型
model = LogisticRegression()
model.fit(X, y)

# 预测概率
probabilities = model.predict_proba(X[:5])
print("前5个样本的预测概率:")
print(probabilities)
```

### 2.3.3 决策树（Decision Tree）

**原理**：通过树形结构进行决策

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# 训练决策树
model = DecisionTreeClassifier(max_depth=3)
model.fit(X_train, y_train)

# 可视化决策树
plt.figure(figsize=(15, 10))
plot_tree(model, filled=True, feature_names=iris.feature_names,
         class_names=iris.target_names)
plt.show()
```

### 2.3.4 随机森林（Random Forest）

**原理**：集成多棵决策树，投票决定结果

```python
from sklearn.ensemble import RandomForestClassifier

# 训练随机森林
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 特征重要性
importances = model.feature_importances_
for name, importance in zip(iris.feature_names, importances):
    print(f"{name}: {importance:.4f}")
```

### 2.3.5 支持向量机（SVM）

**原理**：寻找最优分类超平面

```python
from sklearn.svm import SVC

# 训练SVM
model = SVC(kernel='rbf', C=1.0)
model.fit(X_train, y_train)

# 预测
accuracy = model.score(X_test, y_test)
print(f"SVM准确率: {accuracy*100:.2f}%")
```

### 2.3.6 K近邻（K-Nearest Neighbors）

**原理**：根据最近的K个邻居投票决定类别

```python
from sklearn.neighbors import KNeighborsClassifier

# 训练KNN
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

# 预测
predictions = model.predict(X_test)
```

---

## 2.4 机器学习工作流程

### 标准流程

```
1. 问题定义
    ↓
2. 数据收集
    ↓
3. 数据探索与清洗
    ↓
4. 特征工程
    ↓
5. 模型选择
    ↓
6. 模型训练
    ↓
7. 模型评估
    ↓
8. 模型优化
    ↓
9. 模型部署
    ↓
10. 监控与维护
```

---

## 2.5 实战案例：房价预测系统

### 案例背景

开发一个房价预测模型，根据房屋的特征（面积、房间数、地理位置等）预测房价。

### 完整代码实现

```python
# 房价预测完整项目
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# 设置中文显示
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ====== 1. 数据加载 ======
# 使用波士顿房价数据集（示例）
from sklearn.datasets import fetch_california_housing

data = fetch_california_housing()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['PRICE'] = data.target

print("数据集形状:", df.shape)
print("\n前5行数据:")
print(df.head())

# ====== 2. 数据探索 ======
print("\n数据统计信息:")
print(df.describe())

print("\n缺失值检查:")
print(df.isnull().sum())

# 相关性分析
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('特征相关性热力图')
plt.tight_layout()
plt.savefig('correlation_heatmap.png')
plt.show()

# ====== 3. 特征工程 ======
# 分离特征和目标变量
X = df.drop('PRICE', axis=1)
y = df['PRICE']

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 特征标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\n训练集大小: {X_train.shape}")
print(f"测试集大小: {X_test.shape}")

# ====== 4. 模型训练与对比 ======
models = {
    '线性回归': LinearRegression(),
    'Ridge回归': Ridge(alpha=1.0),
    'Lasso回归': Lasso(alpha=0.1),
    '随机森林': RandomForestRegressor(n_estimators=100, random_state=42),
    '梯度提升': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

results = {}

for name, model in models.items():
    print(f"\n{'='*50}")
    print(f"训练模型: {name}")
    print('='*50)
    
    # 训练
    if name in ['线性回归', 'Ridge回归', 'Lasso回归']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
    
    # 评估
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    results[name] = {
        'RMSE': rmse,
        'MAE': mae,
        'R2': r2,
        'model': model
    }
    
    print(f"均方根误差 (RMSE): {rmse:.4f}")
    print(f"平均绝对误差 (MAE): {mae:.4f}")
    print(f"R² 分数: {r2:.4f}")

# ====== 5. 结果可视化 ======
# 模型性能对比
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

metrics = ['RMSE', 'MAE', 'R2']
for idx, metric in enumerate(metrics):
    values = [results[model][metric] for model in models.keys()]
    axes[idx].bar(models.keys(), values, color='skyblue')
    axes[idx].set_title(f'{metric} 对比')
    axes[idx].set_xlabel('模型')
    axes[idx].set_ylabel(metric)
    axes[idx].tick_params(axis='x', rotation=45)
    
    # 在柱子上显示数值
    for i, v in enumerate(values):
        axes[idx].text(i, v, f'{v:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig('model_comparison.png')
plt.show()

# 选择最佳模型（R²最高）
best_model_name = max(results, key=lambda x: results[x]['R2'])
best_model = results[best_model_name]['model']

print(f"\n{'='*50}")
print(f"最佳模型: {best_model_name}")
print(f"R² 分数: {results[best_model_name]['R2']:.4f}")
print('='*50)

# ====== 6. 预测 vs 实际值可视化 ======
if best_model_name in ['线性回归', 'Ridge回归', 'Lasso回归']:
    y_pred_best = best_model.predict(X_test_scaled)
else:
    y_pred_best = best_model.predict(X_test)

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_best, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
         'r--', lw=2, label='完美预测线')
plt.xlabel('实际房价')
plt.ylabel('预测房价')
plt.title(f'{best_model_name} - 预测 vs 实际')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('prediction_vs_actual.png')
plt.show()

# ====== 7. 特征重要性分析（针对随机森林） ======
if best_model_name == '随机森林':
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance, x='importance', y='feature')
    plt.title('特征重要性排名')
    plt.xlabel('重要性')
    plt.tight_layout()
    plt.savefig('feature_importance.png')
    plt.show()
    
    print("\n特征重要性:")
    print(feature_importance)

# ====== 8. 实用预测函数 ======
class HousePricePredictor:
    def __init__(self, model, scaler, feature_names, is_scaled=False):
        self.model = model
        self.scaler = scaler
        self.feature_names = feature_names
        self.is_scaled = is_scaled
    
    def predict_single(self, house_features):
        """
        预测单个房屋价格
        
        参数:
            house_features: dict, 房屋特征字典
        返回:
            预测价格
        """
        # 转换为DataFrame
        df_input = pd.DataFrame([house_features])
        
        # 确保列顺序正确
        df_input = df_input[self.feature_names]
        
        # 标准化（如果需要）
        if self.is_scaled:
            df_input = self.scaler.transform(df_input)
        
        # 预测
        price = self.model.predict(df_input)[0]
        
        return price
    
    def batch_predict(self, houses_list):
        """批量预测"""
        predictions = []
        for house in houses_list:
            price = self.predict_single(house)
            predictions.append(price)
        return predictions

# 创建预测器
predictor = HousePricePredictor(
    model=best_model,
    scaler=scaler,
    feature_names=X.columns,
    is_scaled=(best_model_name in ['线性回归', 'Ridge回归', 'Lasso回归'])
)

# 示例预测
example_house = {
    'MedInc': 3.5,
    'HouseAge': 15,
    'AveRooms': 6,
    'AveBedrms': 1.2,
    'Population': 1000,
    'AveOccup': 3,
    'Latitude': 37.5,
    'Longitude': -122.0
}

predicted_price = predictor.predict_single(example_house)
print(f"\n示例房屋预测价格: ${predicted_price*100000:.2f}")

# ====== 9. 模型保存 ======
import joblib

joblib.dump(best_model, 'house_price_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
print("\n模型已保存!")

# ====== 10. 模型加载与使用 ======
loaded_model = joblib.load('house_price_model.pkl')
loaded_scaler = joblib.load('scaler.pkl')

print("\n模型加载成功，可以投入使用!")
```

### 输出示例

```
数据集形状: (20640, 9)

模型性能对比:
==================================================
线性回归 - RMSE: 0.7344, R²: 0.5757
Ridge回归 - RMSE: 0.7344, R²: 0.5757
Lasso回归 - RMSE: 0.7376, R²: 0.5720
随机森林 - RMSE: 0.5234, R²: 0.8021  ⭐ 最佳
梯度提升 - RMSE: 0.5567, R²: 0.7764
==================================================

最佳模型: 随机森林
R² 分数: 0.8021

示例房屋预测价格: $234,500.00
```

---

## 2.6 模型评估指标

### 回归问题指标

| 指标 | 公式 | 含义 | 取值范围 |
|------|------|------|----------|
| **MAE** | $\frac{1}{n}\sum\|y_i - \hat{y}_i\|$ | 平均绝对误差 | [0, +∞) |
| **MSE** | $\frac{1}{n}\sum(y_i - \hat{y}_i)^2$ | 均方误差 | [0, +∞) |
| **RMSE** | $\sqrt{MSE}$ | 均方根误差 | [0, +∞) |
| **R²** | $1 - \frac{SS_{res}}{SS_{tot}}$ | 决定系数 | (-∞, 1] |

### 分类问题指标

| 指标 | 计算方式 | 适用场景 |
|------|----------|----------|
| **准确率** | 正确预测数 / 总数 | 类别均衡时 |
| **精确率** | TP / (TP + FP) | 关注误报 |
| **召回率** | TP / (TP + FN) | 关注漏报 |
| **F1分数** | 2 × (精确率 × 召回率) / (精确率 + 召回率) | 综合评估 |

```python
from sklearn.metrics import classification_report, confusion_matrix

# 分类报告
print(classification_report(y_test, y_pred))

# 混淆矩阵
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('混淆矩阵')
plt.show()
```

---

## 2.7 过拟合与欠拟合

### 概念

```
欠拟合          刚刚好          过拟合
(Underfitting)  (Good Fit)     (Overfitting)
    
简单模型        合适模型        复杂模型
训练误差大      训练误差小      训练误差极小
测试误差大      测试误差小      测试误差大
```

### 解决方法

**欠拟合**：
- 增加模型复杂度
- 增加特征
- 减少正则化

**过拟合**：
- 获取更多数据
- 特征选择
- 正则化（L1、L2）
- Early Stopping
- Dropout（深度学习）

```python
# 使用交叉验证检测过拟合
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"交叉验证R²分数: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
```

---

## 📚 本章小结

- 机器学习分为监督学习、非监督学习、强化学习三大类
- 监督学习包括分类和回归任务
- 常见算法：线性回归、逻辑回归、决策树、随机森林、SVM、KNN
- 完整的ML流程：数据→特征→模型→评估→部署
- 通过房价预测案例掌握了实战流程
- 需要注意过拟合和欠拟合问题

---

## 🎯 练习题

1. **概念题**：解释监督学习和非监督学习的区别，各举3个应用例子
2. **实践题**：运行房价预测案例，尝试添加新特征提升模型性能
3. **对比题**：在同一数据集上对比至少3种算法，分析各自优缺点
4. **思考题**：如何判断模型是过拟合还是欠拟合？如何解决？

---

[⬅️ 上一章](./01-AI基础概念.md) | [返回目录](../README.md) | [下一章：Python编程基础 ➡️](./03-Python编程基础.md)
