# 第13章：大语言模型应用开发

## 📝 本章目标

- 掌握OpenAI API和主流大模型API的使用
- 学习LangChain框架进行LLM应用开发
- 精通Prompt Engineering技巧
- 开发完整的RAG知识库问答系统
- 实现企业级AI应用

---

## 13.1 大语言模型概述

### 13.1.1 主流大模型对比

| 模型 | 开发商 | 参数量 | 特点 | 适用场景 |
|------|--------|--------|------|----------|
| **GPT-4** | OpenAI | 未公开 | 最强综合能力 | 复杂任务、多模态 |
| **Claude 3** | Anthropic | 未公开 | 长上下文、安全性 | 文档分析、对话 |
| **文心一言** | 百度 | 260B | 中文优化 | 中文场景 |
| **通义千问** | 阿里 | - | 多模态 | 电商、办公 |
| **GLM-4** | 智谱AI | - | 开源友好 | 私有化部署 |

### 13.1.2 大模型能力

```
文本生成 → 内容创作、代码生成、翻译
文本理解 → 情感分析、信息抽取、分类
对话交互 → 客服机器人、虚拟助手
知识问答 → RAG系统、智能搜索
推理能力 → 数学解题、逻辑推理
```

---

## 13.2 OpenAI API 实战

### 13.2.1 环境配置

```python
# 安装依赖
# pip install openai python-dotenv

import os
from openai import OpenAI
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()

# 初始化客户端
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
)
```

### 13.2.2 基础对话

```python
def chat_completion(messages, model="gpt-3.5-turbo", temperature=0.7):
    """
    基础对话功能
    
    Args:
        messages: 消息列表
        model: 模型名称
        temperature: 温度参数 (0-2)
    """
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature
    )
    
    return response.choices[0].message.content

# 使用示例
messages = [
    {"role": "system", "content": "你是一个专业的Python编程助手"},
    {"role": "user", "content": "如何用Python读取CSV文件？"}
]

answer = chat_completion(messages)
print(answer)
```

### 13.2.3 流式输出

```python
def chat_completion_stream(messages, model="gpt-3.5-turbo"):
    """流式输出，逐字返回"""
    stream = client.chat.completions.create(
        model=model,
        messages=messages,
        stream=True
    )
    
    full_response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            content = chunk.choices[0].delta.content
            full_response += content
            print(content, end="", flush=True)
    
    print()  # 换行
    return full_response

# 使用示例
messages = [
    {"role": "user", "content": "写一个排序算法的Python实现"}
]

response = chat_completion_stream(messages)
```

### 13.2.4 函数调用（Function Calling）

```python
import json

# 定义工具函数
def get_weather(location, unit="celsius"):
    """获取天气信息（模拟）"""
    weather_data = {
        "北京": {"temperature": 25, "condition": "晴朗"},
        "上海": {"temperature": 28, "condition": "多云"},
        "深圳": {"temperature": 32, "condition": "炎热"}
    }
    
    data = weather_data.get(location, {"temperature": 20, "condition": "未知"})
    return json.dumps(data, ensure_ascii=False)

# 定义函数描述
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "获取指定城市的天气信息",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "城市名称，例如：北京、上海"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "温度单位"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

def chat_with_function_calling(user_message):
    """带函数调用的对话"""
    messages = [
        {"role": "user", "content": user_message}
    ]
    
    # 第一次调用：模型决定是否调用函数
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        tools=tools,
        tool_choice="auto"
    )
    
    response_message = response.choices[0].message
    messages.append(response_message)
    
    # 如果模型要求调用函数
    if response_message.tool_calls:
        for tool_call in response_message.tool_calls:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)
            
            # 执行函数
            if function_name == "get_weather":
                function_response = get_weather(**function_args)
            
            # 将函数结果添加到消息中
            messages.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": function_response
            })
        
        # 第二次调用：使用函数结果生成最终回复
        second_response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages
        )
        
        return second_response.choices[0].message.content
    
    return response_message.content

# 使用示例
result = chat_with_function_calling("北京今天天气怎么样？")
print(result)
```

### 13.2.5 图像理解（GPT-4 Vision）

```python
def analyze_image(image_url, question="这张图片里有什么？"):
    """图像理解"""
    response = client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {"type": "image_url", "image_url": {"url": image_url}}
                ]
            }
        ],
        max_tokens=300
    )
    
    return response.choices[0].message.content

# 使用示例
result = analyze_image(
    "https://example.com/image.jpg",
    "详细描述这张图片的内容"
)
print(result)
```

---

## 13.3 Prompt Engineering 提示词工程

### 13.3.1 核心原则

**1. 清晰明确**

```python
# ❌ 不好的提示词
prompt = "帮我写点东西"

# ✅ 好的提示词
prompt = """
请你作为一名专业的营销文案撰写者，为一款新上市的智能手表撰写产品介绍。

要求：
- 字数：200-300字
- 重点突出健康监测和运动追踪功能
- 目标受众：25-40岁的都市白领
- 语言风格：专业但不失亲和力
"""
```

**2. 提供示例（Few-shot Learning）**

```python
prompt = """
任务：将产品评论分类为正面、负面或中性。

示例：
评论：这个产品质量很好，非常满意！
分类：正面

评论：价格太贵了，性价比不高。
分类：负面

评论：还行吧，凑合能用。
分类：中性

现在请分类以下评论：
评论：{user_review}
分类：
"""
```

**3. 分步思考（Chain of Thought）**

```python
prompt = """
问题：一个班级有40名学生，其中60%是女生。如果又来了10名学生，其中70%是男生，现在班级里男生和女生各有多少人？

请按以下步骤思考：
1. 计算原来的男生和女生人数
2. 计算新来的男生和女生人数
3. 计算总的男生和女生人数

让我们一步步来解决这个问题：
"""
```

**4. 角色扮演（Role Playing）**

```python
prompt = """
你是一位资深的心理咨询师，拥有20年的临床经验。
你的特点：
- 善于倾听，给人温暖和安全感
- 使用专业但通俗易懂的语言
- 会引导来访者自己思考和发现问题
- 不会轻易下结论，而是提供建设性建议

现在有一位来访者说：{user_input}

请给予专业的回应。
"""
```

### 13.3.2 高级技巧

**思维链（CoT）+ 自我一致性**

```python
def solve_with_consistency(problem, n_attempts=3):
    """
    使用自我一致性提高推理准确性
    """
    prompt_template = """
    问题：{problem}
    
    请仔细思考并一步步解决这个问题。
    """
    
    answers = []
    for _ in range(n_attempts):
        messages = [
            {"role": "user", "content": prompt_template.format(problem=problem)}
        ]
        answer = chat_completion(messages, temperature=0.7)
        answers.append(answer)
    
    # 投票选择最常见的答案
    from collections import Counter
    # 简化：这里应该提取答案的关键部分进行比较
    final_answer = Counter(answers).most_common(1)[0][0]
    
    return final_answer, answers
```

**ReAct模式（推理+行动）**

```python
class ReActAgent:
    """ReAct (Reasoning + Acting) 智能体"""
    
    def __init__(self):
        self.tools = {
            "search": self.search_tool,
            "calculate": self.calculate_tool,
            "finish": self.finish_tool
        }
        self.history = []
    
    def search_tool(self, query):
        """模拟搜索工具"""
        return f"搜索结果: 关于'{query}'的信息..."
    
    def calculate_tool(self, expression):
        """计算工具"""
        try:
            result = eval(expression)
            return f"计算结果: {result}"
        except:
            return "计算错误"
    
    def finish_tool(self, answer):
        """完成任务"""
        return f"最终答案: {answer}"
    
    def run(self, task, max_steps=5):
        """执行任务"""
        prompt = f"""
你是一个能够进行推理和行动的智能助手。

可用工具：
- search(query): 搜索信息
- calculate(expression): 计算数学表达式
- finish(answer): 给出最终答案

任务: {task}

请按以下格式思考和行动：
Thought: [你的思考]
Action: [工具名称]
Action Input: [工具输入]
Observation: [工具输出]
... (重复直到得出答案)
Thought: 我知道最终答案了
Action: finish
Action Input: [最终答案]
"""
        
        messages = [{"role": "user", "content": prompt}]
        
        for step in range(max_steps):
            response = chat_completion(messages)
            self.history.append(response)
            
            # 解析行动
            if "Action:" in response:
                action_line = [l for l in response.split('\n') if l.startswith("Action:")][0]
                action = action_line.split(":")[1].strip()
                
                input_line = [l for l in response.split('\n') if l.startswith("Action Input:")][0]
                action_input = input_line.split(":")[1].strip()
                
                # 执行行动
                if action in self.tools:
                    observation = self.tools[action](action_input)
                    
                    if action == "finish":
                        return observation
                    
                    # 添加观察结果
                    messages.append({"role": "assistant", "content": response})
                    messages.append({"role": "user", "content": f"Observation: {observation}"})
        
        return "未能在规定步数内完成任务"

# 使用示例
agent = ReActAgent()
result = agent.run("2024年世界杯冠军是谁？该国人口是多少？")
print(result)
```

---

## 13.4 LangChain 框架实战

### 13.4.1 基础使用

```python
# pip install langchain langchain-openai

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser

# 1. 初始化模型
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.7,
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# 2. 创建提示词模板
prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一个专业的{profession}"),
    ("human", "{input}")
])

# 3. 创建链
chain = prompt | llm | StrOutputParser()

# 4. 执行
result = chain.invoke({
    "profession": "Python程序员",
    "input": "如何优化列表推导式的性能？"
})

print(result)
```

### 13.4.2 记忆管理

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# 创建记忆
memory = ConversationBufferMemory()

# 创建对话链
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# 多轮对话
response1 = conversation.predict(input="我叫张三，是一名数据分析师")
print(response1)

response2 = conversation.predict(input="我的职业是什么？")
print(response2)  # 模型会记住之前说的职业

# 查看历史
print(memory.load_memory_variables({}))
```

### 13.4.3 文档处理

```python
from langchain.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# 1. 加载文档
loader = TextLoader("document.txt", encoding="utf-8")
documents = loader.load()

# 2. 文本分割
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)
chunks = text_splitter.split_documents(documents)

# 3. 创建向量数据库
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)

# 4. 相似度搜索
query = "什么是机器学习？"
docs = vectorstore.similarity_search(query, k=3)

for doc in docs:
    print(doc.page_content)
    print("-" * 50)
```

---

## 13.5 完整案例：RAG知识库问答系统

### 13.5.1 系统架构

```
文档上传 → 文本分割 → 向量化 → 存储到向量数据库
                                    ↓
用户提问 → 向量化 → 相似度检索 → 整合上下文 → LLM生成回答
```

### 13.5.2 完整实现

```python
# RAG 知识库问答系统完整实现
import os
from typing import List
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.document_loaders import (
    DirectoryLoader, TextLoader, PyPDFLoader, 
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import gradio as gr

class RAGKnowledgeBase:
    """RAG知识库系统"""
    
    def __init__(self, persist_directory="./knowledge_base"):
        self.persist_directory = persist_directory
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.3
        )
        self.vectorstore = None
        self.qa_chain = None
    
    def load_documents(self, directory: str) -> List:
        """
        加载多种格式的文档
        
        Args:
            directory: 文档目录路径
        """
        documents = []
        
        # 加载 TXT 文件
        txt_loader = DirectoryLoader(
            directory,
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        documents.extend(txt_loader.load())
        
        # 加载 PDF 文件
        pdf_loader = DirectoryLoader(
            directory,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader
        )
        documents.extend(pdf_loader.load())
        
        # 加载 Markdown 文件
        md_loader = DirectoryLoader(
            directory,
            glob="**/*.md",
            loader_cls=UnstructuredMarkdownLoader
        )
        documents.extend(md_loader.load())
        
        print(f"成功加载 {len(documents)} 个文档")
        return documents
    
    def split_documents(self, documents: List) -> List:
        """
        智能文本分割
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", "。", "！", "？", "，", " ", ""]
        )
        
        chunks = text_splitter.split_documents(documents)
        print(f"文档被分割为 {len(chunks)} 个片段")
        return chunks
    
    def build_vectorstore(self, chunks: List):
        """
        构建向量数据库
        """
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )
        self.vectorstore.persist()
        print("向量数据库构建完成")
    
    def load_vectorstore(self):
        """
        加载已有的向量数据库
        """
        self.vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings
        )
        print("向量数据库加载完成")
    
    def create_qa_chain(self):
        """
        创建问答链
        """
        # 自定义提示词
        prompt_template = """
你是一个专业的知识库助手，请基于以下提供的上下文信息回答用户的问题。

要求：
1. 只使用提供的上下文信息回答，不要编造内容
2. 如果上下文中没有相关信息，请明确告知用户
3. 回答要准确、简洁、易懂
4. 可以适当展开说明，但要确保信息来源于上下文

上下文信息：
{context}

用户问题：{question}

请给出专业的回答：
"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # 创建检索式问答链
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}
            ),
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        print("问答链创建完成")
    
    def query(self, question: str) -> dict:
        """
        查询知识库
        
        Args:
            question: 用户问题
        
        Returns:
            包含答案和来源文档的字典
        """
        if self.qa_chain is None:
            raise ValueError("请先创建问答链")
        
        result = self.qa_chain({"query": question})
        
        return {
            "answer": result["result"],
            "source_documents": result["source_documents"]
        }
    
    def initialize_from_directory(self, documents_dir: str):
        """
        从文档目录初始化知识库
        """
        # 1. 加载文档
        documents = self.load_documents(documents_dir)
        
        # 2. 分割文档
        chunks = self.split_documents(documents)
        
        # 3. 构建向量数据库
        self.build_vectorstore(chunks)
        
        # 4. 创建问答链
        self.create_qa_chain()
    
    def initialize_from_existing(self):
        """
        从已有向量数据库初始化
        """
        # 1. 加载向量数据库
        self.load_vectorstore()
        
        # 2. 创建问答链
        self.create_qa_chain()

# ====== Web 界面 ======
def create_web_interface(kb: RAGKnowledgeBase):
    """
    创建Gradio Web界面
    """
    def chat_interface(message, history):
        """聊天接口"""
        try:
            result = kb.query(message)
            answer = result["answer"]
            
            # 添加来源信息
            sources = result["source_documents"]
            if sources:
                answer += "\n\n📚 参考来源："
                for idx, doc in enumerate(sources[:2], 1):
                    source = doc.metadata.get('source', '未知')
                    answer += f"\n{idx}. {source}"
            
            return answer
        except Exception as e:
            return f"❌ 发生错误: {str(e)}"
    
    # 创建界面
    demo = gr.ChatInterface(
        fn=chat_interface,
        title="🤖 AI知识库问答系统",
        description="基于RAG技术的智能问答助手",
        examples=[
            "什么是机器学习？",
            "深度学习和机器学习有什么区别？",
            "如何训练一个神经网络？"
        ],
        theme=gr.themes.Soft(),
        retry_btn="🔄 重试",
        undo_btn="↩️ 撤销",
        clear_btn="🗑️ 清空"
    )
    
    return demo

# ====== 命令行界面 ======
def cli_interface(kb: RAGKnowledgeBase):
    """
    命令行交互界面
    """
    print("\n" + "="*50)
    print("🤖 AI知识库问答系统")
    print("="*50)
    print("输入 'quit' 或 'exit' 退出")
    print("输入 'clear' 清空屏幕")
    print("="*50 + "\n")
    
    while True:
        question = input("\n💬 您的问题: ").strip()
        
        if question.lower() in ['quit', 'exit']:
            print("\n👋 再见！")
            break
        
        if question.lower() == 'clear':
            os.system('cls' if os.name == 'nt' else 'clear')
            continue
        
        if not question:
            continue
        
        print("\n🤔 思考中...")
        
        try:
            result = kb.query(question)
            
            print("\n" + "="*50)
            print("📝 回答:")
            print("="*50)
            print(result["answer"])
            
            # 显示来源
            if result["source_documents"]:
                print("\n" + "-"*50)
                print("📚 参考来源:")
                for idx, doc in enumerate(result["source_documents"], 1):
                    source = doc.metadata.get('source', '未知')
                    print(f"{idx}. {source}")
                    print(f"   内容片段: {doc.page_content[:100]}...")
        
        except Exception as e:
            print(f"\n❌ 错误: {str(e)}")

# ====== 主程序 ======
if __name__ == "__main__":
    # 初始化知识库
    kb = RAGKnowledgeBase(persist_directory="./chroma_db")
    
    # 选择初始化方式
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--init":
        # 从文档目录初始化
        documents_dir = input("请输入文档目录路径: ").strip()
        if os.path.exists(documents_dir):
            kb.initialize_from_directory(documents_dir)
        else:
            print("❌ 目录不存在")
            exit(1)
    else:
        # 从已有数据库加载
        if os.path.exists("./chroma_db"):
            kb.initialize_from_existing()
        else:
            print("❌ 未找到已有知识库，请先使用 --init 参数初始化")
            exit(1)
    
    # 选择界面类型
    interface_type = input("\n选择界面类型 (web/cli) [默认: web]: ").strip().lower()
    
    if interface_type == "cli":
        cli_interface(kb)
    else:
        demo = create_web_interface(kb)
        demo.launch(share=False, server_name="0.0.0.0", server_port=7860)
```

### 13.5.3 使用示例

```bash
# 1. 初次使用：构建知识库
python rag_system.py --init
# 输入文档目录: ./documents

# 2. 后续使用：直接加载
python rag_system.py

# 3. 使用命令行界面
python rag_system.py
# 选择: cli
```

### 13.5.4 性能优化

```python
# 使用向量数据库进行混合检索
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import BM25Retriever

def create_hybrid_retriever(vectorstore, documents):
    """
    创建混合检索器（向量 + BM25）
    """
    # 向量检索器
    vector_retriever = vectorstore.as_retriever(
        search_kwargs={"k": 4}
    )
    
    # BM25检索器
    bm25_retriever = BM25Retriever.from_documents(documents)
    bm25_retriever.k = 4
    
    # 混合检索器
    ensemble_retriever = EnsembleRetriever(
        retrievers=[vector_retriever, bm25_retriever],
        weights=[0.6, 0.4]
    )
    
    return ensemble_retriever
```

---

## 13.6 企业级应用实践

### 13.6.1 智能客服系统

```python
class CustomerServiceBot:
    """智能客服机器人"""
    
    def __init__(self, kb: RAGKnowledgeBase):
        self.kb = kb
        self.conversation_history = {}
    
    def handle_query(self, user_id: str, message: str) -> str:
        """
        处理用户查询
        """
        # 获取历史对话
        history = self.conversation_history.get(user_id, [])
        
        # 意图识别
        intent = self.classify_intent(message)
        
        if intent == "greeting":
            return "您好！我是智能客服助手，有什么可以帮您的吗？"
        
        elif intent == "farewell":
            return "感谢您的咨询，祝您生活愉快！"
        
        elif intent == "question":
            # 使用知识库回答
            result = self.kb.query(message)
            answer = result["answer"]
            
            # 添加友好的回复
            response = f"根据我们的资料库，{answer}\n\n还有其他问题吗？"
            
            # 保存历史
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": response})
            self.conversation_history[user_id] = history[-10:]  # 保留最近10轮
            
            return response
        
        else:
            return "抱歉，我没有理解您的问题。能否换个说法？"
    
    def classify_intent(self, message: str) -> str:
        """简单的意图分类"""
        greetings = ["你好", "您好", "hi", "hello"]
        farewells = ["再见", "拜拜", "bye"]
        
        message_lower = message.lower()
        
        if any(g in message_lower for g in greetings):
            return "greeting"
        elif any(f in message_lower for f in farewells):
            return "farewell"
        else:
            return "question"
```

---

## 📚 本章小结

- 掌握了OpenAI API的核心功能：对话、流式输出、函数调用
- 学习了Prompt Engineering的关键技巧
- 使用LangChain构建复杂的AI应用
- 开发了完整的RAG知识库问答系统
- 了解了企业级AI应用的实践方法

---

## 🎯 练习题

1. **实践题**：基于本章的RAG系统，添加对Word文档的支持
2. **优化题**：实现对话历史的持久化存储（使用Redis或SQLite）
3. **扩展题**：添加多轮对话能力，让系统能够理解上下文
4. **项目题**：开发一个基于RAG的个人知识管理助手

---

[⬅️ 上一章](./12-PyTorch实战.md) | [返回目录](../README.md) | [下一章：模型部署 ➡️](./14-模型部署.md)
