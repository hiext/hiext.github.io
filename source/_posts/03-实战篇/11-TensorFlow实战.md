# ç¬¬11ç« ï¼šTensorFlowå®æˆ˜

## ğŸ“ æœ¬ç« ç›®æ ‡
- æŒæ¡TensorFlow 2.xæ ¸å¿ƒAPI
- å­¦ä¹ Kerasé«˜çº§åŠŸèƒ½
- å®ç°å®Œæ•´è®­ç»ƒæµç¨‹
- æŒæ¡æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

## 11.1 TensorFlowåŸºç¡€

```python
import tensorflow as tf

# å¼ é‡åˆ›å»º
a = tf.constant([1, 2, 3])
b = tf.Variable([4, 5, 6])
c = tf.zeros((3, 3))
d = tf.ones((2, 2))

# å¼ é‡è¿ç®—
x = tf.constant([[1, 2], [3, 4]])
y = tf.constant([[5, 6], [7, 8]])

print(tf.add(x, y))
print(tf.matmul(x, y))
print(tf.reduce_mean(x))
```

## 11.2 Keras Sequential API

```python
from tensorflow import keras
from tensorflow.keras import layers

# Sequentialæ¨¡å‹
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(784,)),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# è®­ç»ƒ
model.fit(x_train, y_train, epochs=10, batch_size=32)
```

## 11.3 Functional API

```python
# å¤šè¾“å…¥å¤šè¾“å‡ºæ¨¡å‹
input1 = keras.Input(shape=(784,))
input2 = keras.Input(shape=(10,))

x1 = layers.Dense(64, activation='relu')(input1)
x2 = layers.Dense(32, activation='relu')(input2)

merged = layers.concatenate([x1, x2])
output = layers.Dense(10, activation='softmax')(merged)

model = keras.Model(inputs=[input1, input2], outputs=output)
```

## 11.4 è‡ªå®šä¹‰å±‚å’Œæ¨¡å‹

```python
class CustomLayer(layers.Layer):
    def __init__(self, units):
        super(CustomLayer, self).__init__()
        self.units = units
    
    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer='random_normal',
            trainable=True
        )
        self.b = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            trainable=True
        )
    
    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

# è‡ªå®šä¹‰æ¨¡å‹
class CustomModel(keras.Model):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.dense1 = layers.Dense(64, activation='relu')
        self.dense2 = layers.Dense(10, activation='softmax')
    
    def call(self, inputs):
        x = self.dense1(inputs)
        return self.dense2(x)
```

## 11.5 è®­ç»ƒå¾ªç¯

```python
# è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯
@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        predictions = model(x, training=True)
        loss = loss_fn(y, predictions)
    
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    
    train_acc_metric(y, predictions)
    return loss

# è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    for step, (x_batch, y_batch) in enumerate(train_dataset):
        loss = train_step(x_batch, y_batch)
    
    print(f'Epoch {epoch}, Loss: {loss:.4f}')
```

## 11.6 æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# ä¿å­˜æ•´ä¸ªæ¨¡å‹
model.save('my_model.h5')
model = keras.models.load_model('my_model.h5')

# åªä¿å­˜æƒé‡
model.save_weights('my_weights.h5')
model.load_weights('my_weights.h5')

# SavedModelæ ¼å¼
model.save('saved_model/')
model = keras.models.load_model('saved_model/')
```

## 11.7 TensorBoardå¯è§†åŒ–

```python
# TensorBoardå›è°ƒ
tensorboard_callback = keras.callbacks.TensorBoard(
    log_dir='./logs',
    histogram_freq=1
)

model.fit(
    x_train, y_train,
    epochs=10,
    callbacks=[tensorboard_callback]
)

# å¯åŠ¨TensorBoard
# tensorboard --logdir=./logs
```

## ğŸ“š æœ¬ç« å°ç»“
- âœ… TensorFlowåŸºç¡€æ“ä½œ
- âœ… Keras API(Sequential, Functional)
- âœ… è‡ªå®šä¹‰å±‚å’Œæ¨¡å‹
- âœ… è®­ç»ƒå¾ªç¯å®ç°
- âœ… æ¨¡å‹ä¿å­˜å’Œå¯è§†åŒ–

[â¬…ï¸ ä¸Šä¸€ç« ](../02-è¿›é˜¶ç¯‡/10-è‡ªç„¶è¯­è¨€å¤„ç†.md) | [è¿”å›ç›®å½•](../README.md) | [ä¸‹ä¸€ç«  â¡ï¸](./12-PyTorchå®æˆ˜.md)
