# 第14章：AI模型部署

## 📝 本章目标
- 掌握模型部署流程
- 学习Flask/FastAPI部署
- 了解Docker容器化
- 实现云平台部署

## 14.1 Flask部署

```python
from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)
model = joblib.load('model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    prediction = model.predict([data['features']])
    return jsonify({'prediction': prediction.tolist()})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

## 14.2 FastAPI部署

```python
from fastapi import FastAPI
from pydantic import BaseModel
import torch

app = FastAPI()

class PredictionInput(BaseModel):
    features: list

@app.post("/predict")
async def predict(input_data: PredictionInput):
    # 加载模型
    model = torch.load('model.pth')
    prediction = model(torch.tensor(input_data.features))
    return {"prediction": prediction.tolist()}
```

## 14.3 Docker部署

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## 14.4 模型优化

```python
# TensorFlow Lite
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
    f.write(tflite_model)

# ONNX导出
import torch.onnx

dummy_input = torch.randn(1, 3, 224, 224)
torch.onnx.export(model, dummy_input, "model.onnx")
```

## 📚 本章小结
- ✅ Flask/FastAPI部署
- ✅ Docker容器化
- ✅ 模型优化技术
- ✅ 云平台部署方案

[⬅️ 上一章](./13-大语言模型应用.md) | [返回目录](../README.md) | [下一章 ➡️](./15-MLOps工程化.md)
