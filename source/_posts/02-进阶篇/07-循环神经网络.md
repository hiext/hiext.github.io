# 第7章：循环神经网络(RNN/LSTM)

## 📝 本章目标
- 理解RNN网络结构
- 掌握LSTM和GRU原理
- 学习序列数据处理
- 实现股票预测项目

## 7.1 RNN基础

### 7.1.1 RNN原理

```python
import numpy as np

class SimpleRNN:
    def __init__(self, input_size, hidden_size, output_size):
        self.hidden_size = hidden_size
        
        # 权重初始化
        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01
        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.Why = np.random.randn(output_size, hidden_size) * 0.01
        self.bh = np.zeros((hidden_size, 1))
        self.by = np.zeros((output_size, 1))
    
    def forward(self, inputs):
        """前向传播"""
        h = np.zeros((self.hidden_size, 1))
        self.last_inputs = inputs
        self.last_hs = {0: h}
        
        # 处理序列
        for i, x in enumerate(inputs):
            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)
            self.last_hs[i + 1] = h
        
        # 输出
        y = self.Why @ h + self.by
        return y, h
    
    def backward(self, d_y, learn_rate=0.001):
        """反向传播"""
        n = len(self.last_inputs)
        
        # 输出层梯度
        d_Why = d_y @ self.last_hs[n].T
        d_by = d_y
        
        # 隐藏层梯度
        d_h = self.Why.T @ d_y
        d_Wxh = np.zeros_like(self.Wxh)
        d_Whh = np.zeros_like(self.Whh)
        d_bh = np.zeros_like(self.bh)
        
        # BPTT
        for t in reversed(range(n)):
            temp = (1 - self.last_hs[t + 1] ** 2) * d_h
            d_bh += temp
            d_Wxh += temp @ self.last_inputs[t].T
            d_Whh += temp @ self.last_hs[t].T
            d_h = self.Whh.T @ temp
        
        # 更新权重
        self.Wxh -= learn_rate * d_Wxh
        self.Whh -= learn_rate * d_Whh
        self.Why -= learn_rate * d_Why
        self.bh -= learn_rate * d_bh
        self.by -= learn_rate * d_by
```

### 7.1.2 Keras实现RNN

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.SimpleRNN(64, return_sequences=True, input_shape=(None, 10)),
    layers.SimpleRNN(32),
    layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')
```

## 7.2 LSTM网络

### 7.2.1 LSTM结构

```python
class LSTM:
    """LSTM单元实现"""
    
    def __init__(self, input_size, hidden_size):
        self.hidden_size = hidden_size
        
        # 遗忘门
        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)
        self.bf = np.zeros((hidden_size, 1))
        
        # 输入门
        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)
        self.bi = np.zeros((hidden_size, 1))
        
        # 候选值
        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)
        self.bc = np.zeros((hidden_size, 1))
        
        # 输出门
        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)
        self.bo = np.zeros((hidden_size, 1))
    
    def forward(self, x, h_prev, c_prev):
        """LSTM前向传播"""
        # 拼接输入
        combined = np.vstack([h_prev, x])
        
        # 遗忘门
        f = self.sigmoid(self.Wf @ combined + self.bf)
        
        # 输入门
        i = self.sigmoid(self.Wi @ combined + self.bi)
        
        # 候选值
        c_tilde = np.tanh(self.Wc @ combined + self.bc)
        
        # 更新细胞状态
        c = f * c_prev + i * c_tilde
        
        # 输出门
        o = self.sigmoid(self.Wo @ combined + self.bo)
        
        # 隐藏状态
        h = o * np.tanh(c)
        
        return h, c
    
    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

# Keras LSTM
model = keras.Sequential([
    layers.LSTM(128, return_sequences=True, input_shape=(None, 1)),
    layers.Dropout(0.2),
    layers.LSTM(64),
    layers.Dense(1)
])
```

## 7.3 GRU网络

```python
class GRU:
    """GRU单元"""
    
    def __init__(self, input_size, hidden_size):
        self.hidden_size = hidden_size
        
        # 重置门
        self.Wr = np.random.randn(hidden_size, input_size + hidden_size)
        self.br = np.zeros((hidden_size, 1))
        
        # 更新门
        self.Wz = np.random.randn(hidden_size, input_size + hidden_size)
        self.bz = np.zeros((hidden_size, 1))
        
        # 候选隐藏状态
        self.Wh = np.random.randn(hidden_size, input_size + hidden_size)
        self.bh = np.zeros((hidden_size, 1))
    
    def forward(self, x, h_prev):
        combined = np.vstack([h_prev, x])
        
        # 重置门
        r = self.sigmoid(self.Wr @ combined + self.br)
        
        # 更新门
        z = self.sigmoid(self.Wz @ combined + self.bz)
        
        # 候选隐藏状态
        combined_reset = np.vstack([r * h_prev, x])
        h_tilde = np.tanh(self.Wh @ combined_reset + self.bh)
        
        # 新的隐藏状态
        h = (1 - z) * h_prev + z * h_tilde
        
        return h
    
    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
```

## 7.4 实战：股票价格预测

```python
import pandas as pd
import numpy as np
from tensorflow import keras
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

class StockPredictor:
    """股票价格预测系统"""
    
    def __init__(self, lookback=60):
        self.lookback = lookback
        self.scaler = MinMaxScaler()
        self.model = None
    
    def prepare_data(self, data):
        """准备数据"""
        # 归一化
        scaled_data = self.scaler.fit_transform(data.reshape(-1, 1))
        
        X, y = [], []
        for i in range(self.lookback, len(scaled_data)):
            X.append(scaled_data[i-self.lookback:i, 0])
            y.append(scaled_data[i, 0])
        
        return np.array(X), np.array(y)
    
    def build_model(self):
        """构建LSTM模型"""
        self.model = keras.Sequential([
            layers.LSTM(50, return_sequences=True, 
                       input_shape=(self.lookback, 1)),
            layers.Dropout(0.2),
            layers.LSTM(50, return_sequences=True),
            layers.Dropout(0.2),
            layers.LSTM(50),
            layers.Dropout(0.2),
            layers.Dense(1)
        ])
        
        self.model.compile(optimizer='adam', loss='mse')
    
    def train(self, X_train, y_train, epochs=50, batch_size=32):
        """训练模型"""
        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
        
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.1,
            verbose=1
        )
        
        return history
    
    def predict(self, X_test):
        """预测"""
        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        predictions = self.model.predict(X_test)
        predictions = self.scaler.inverse_transform(predictions)
        return predictions
    
    def plot_results(self, y_true, y_pred):
        """可视化结果"""
        plt.figure(figsize=(14, 6))
        plt.plot(y_true, label='实际价格', color='blue')
        plt.plot(y_pred, label='预测价格', color='red')
        plt.xlabel('时间')
        plt.ylabel('股价')
        plt.title('股票价格预测')
        plt.legend()
        plt.grid(True)
        plt.show()

# 使用示例
# 假设有股票数据
stock_data = np.random.randn(1000) * 10 + 100

# 创建预测器
predictor = StockPredictor(lookback=60)

# 准备数据
X, y = predictor.prepare_data(stock_data)

# 划分训练集和测试集
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# 构建和训练模型
predictor.build_model()
predictor.train(X_train, y_train, epochs=50)

# 预测
predictions = predictor.predict(X_test)
y_test_inv = predictor.scaler.inverse_transform(y_test.reshape(-1, 1))

# 可视化
predictor.plot_results(y_test_inv, predictions)
```

## 7.5 Bi-LSTM双向网络

```python
# 双向LSTM
model = keras.Sequential([
    layers.Bidirectional(
        layers.LSTM(64, return_sequences=True),
        input_shape=(None, 10)
    ),
    layers.Bidirectional(layers.LSTM(32)),
    layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')
```

## 7.6 注意力机制

```python
class Attention(layers.Layer):
    """注意力层"""
    
    def __init__(self, units):
        super(Attention, self).__init__()
        self.W1 = layers.Dense(units)
        self.W2 = layers.Dense(units)
        self.V = layers.Dense(1)
    
    def call(self, query, values):
        # query shape: (batch_size, hidden_size)
        # values shape: (batch_size, seq_len, hidden_size)
        
        # 扩展维度
        query_with_time = tf.expand_dims(query, 1)
        
        # 计算分数
        score = self.V(tf.nn.tanh(
            self.W1(query_with_time) + self.W2(values)
        ))
        
        # 注意力权重
        attention_weights = tf.nn.softmax(score, axis=1)
        
        # 加权求和
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        
        return context_vector, attention_weights
```

## 📚 本章小结
- ✅ RNN基本原理和实现
- ✅ LSTM解决长期依赖问题
- ✅ GRU简化版LSTM
- ✅ 股票预测实战项目
- ✅ 双向LSTM和注意力机制

[⬅️ 上一章](./06-卷积神经网络.md) | [返回目录](../README.md) | [下一章 ➡️](./08-Transformer架构.md)
