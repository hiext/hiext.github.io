# ç¬¬7ç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ(RNN/LSTM)

## ğŸ“ æœ¬ç« ç›®æ ‡
- ç†è§£RNNç½‘ç»œç»“æ„
- æŒæ¡LSTMå’ŒGRUåŸç†
- å­¦ä¹ åºåˆ—æ•°æ®å¤„ç†
- å®ç°è‚¡ç¥¨é¢„æµ‹é¡¹ç›®

## 7.1 RNNåŸºç¡€

### 7.1.1 RNNåŸç†

```python
import numpy as np

class SimpleRNN:
    def __init__(self, input_size, hidden_size, output_size):
        self.hidden_size = hidden_size
        
        # æƒé‡åˆå§‹åŒ–
        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01
        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.Why = np.random.randn(output_size, hidden_size) * 0.01
        self.bh = np.zeros((hidden_size, 1))
        self.by = np.zeros((output_size, 1))
    
    def forward(self, inputs):
        """å‰å‘ä¼ æ’­"""
        h = np.zeros((self.hidden_size, 1))
        self.last_inputs = inputs
        self.last_hs = {0: h}
        
        # å¤„ç†åºåˆ—
        for i, x in enumerate(inputs):
            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)
            self.last_hs[i + 1] = h
        
        # è¾“å‡º
        y = self.Why @ h + self.by
        return y, h
    
    def backward(self, d_y, learn_rate=0.001):
        """åå‘ä¼ æ’­"""
        n = len(self.last_inputs)
        
        # è¾“å‡ºå±‚æ¢¯åº¦
        d_Why = d_y @ self.last_hs[n].T
        d_by = d_y
        
        # éšè—å±‚æ¢¯åº¦
        d_h = self.Why.T @ d_y
        d_Wxh = np.zeros_like(self.Wxh)
        d_Whh = np.zeros_like(self.Whh)
        d_bh = np.zeros_like(self.bh)
        
        # BPTT
        for t in reversed(range(n)):
            temp = (1 - self.last_hs[t + 1] ** 2) * d_h
            d_bh += temp
            d_Wxh += temp @ self.last_inputs[t].T
            d_Whh += temp @ self.last_hs[t].T
            d_h = self.Whh.T @ temp
        
        # æ›´æ–°æƒé‡
        self.Wxh -= learn_rate * d_Wxh
        self.Whh -= learn_rate * d_Whh
        self.Why -= learn_rate * d_Why
        self.bh -= learn_rate * d_bh
        self.by -= learn_rate * d_by
```

### 7.1.2 Keraså®ç°RNN

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.SimpleRNN(64, return_sequences=True, input_shape=(None, 10)),
    layers.SimpleRNN(32),
    layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')
```

## 7.2 LSTMç½‘ç»œ

### 7.2.1 LSTMç»“æ„

```python
class LSTM:
    """LSTMå•å…ƒå®ç°"""
    
    def __init__(self, input_size, hidden_size):
        self.hidden_size = hidden_size
        
        # é—å¿˜é—¨
        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)
        self.bf = np.zeros((hidden_size, 1))
        
        # è¾“å…¥é—¨
        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)
        self.bi = np.zeros((hidden_size, 1))
        
        # å€™é€‰å€¼
        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)
        self.bc = np.zeros((hidden_size, 1))
        
        # è¾“å‡ºé—¨
        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)
        self.bo = np.zeros((hidden_size, 1))
    
    def forward(self, x, h_prev, c_prev):
        """LSTMå‰å‘ä¼ æ’­"""
        # æ‹¼æ¥è¾“å…¥
        combined = np.vstack([h_prev, x])
        
        # é—å¿˜é—¨
        f = self.sigmoid(self.Wf @ combined + self.bf)
        
        # è¾“å…¥é—¨
        i = self.sigmoid(self.Wi @ combined + self.bi)
        
        # å€™é€‰å€¼
        c_tilde = np.tanh(self.Wc @ combined + self.bc)
        
        # æ›´æ–°ç»†èƒçŠ¶æ€
        c = f * c_prev + i * c_tilde
        
        # è¾“å‡ºé—¨
        o = self.sigmoid(self.Wo @ combined + self.bo)
        
        # éšè—çŠ¶æ€
        h = o * np.tanh(c)
        
        return h, c
    
    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

# Keras LSTM
model = keras.Sequential([
    layers.LSTM(128, return_sequences=True, input_shape=(None, 1)),
    layers.Dropout(0.2),
    layers.LSTM(64),
    layers.Dense(1)
])
```

## 7.3 GRUç½‘ç»œ

```python
class GRU:
    """GRUå•å…ƒ"""
    
    def __init__(self, input_size, hidden_size):
        self.hidden_size = hidden_size
        
        # é‡ç½®é—¨
        self.Wr = np.random.randn(hidden_size, input_size + hidden_size)
        self.br = np.zeros((hidden_size, 1))
        
        # æ›´æ–°é—¨
        self.Wz = np.random.randn(hidden_size, input_size + hidden_size)
        self.bz = np.zeros((hidden_size, 1))
        
        # å€™é€‰éšè—çŠ¶æ€
        self.Wh = np.random.randn(hidden_size, input_size + hidden_size)
        self.bh = np.zeros((hidden_size, 1))
    
    def forward(self, x, h_prev):
        combined = np.vstack([h_prev, x])
        
        # é‡ç½®é—¨
        r = self.sigmoid(self.Wr @ combined + self.br)
        
        # æ›´æ–°é—¨
        z = self.sigmoid(self.Wz @ combined + self.bz)
        
        # å€™é€‰éšè—çŠ¶æ€
        combined_reset = np.vstack([r * h_prev, x])
        h_tilde = np.tanh(self.Wh @ combined_reset + self.bh)
        
        # æ–°çš„éšè—çŠ¶æ€
        h = (1 - z) * h_prev + z * h_tilde
        
        return h
    
    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
```

## 7.4 å®æˆ˜ï¼šè‚¡ç¥¨ä»·æ ¼é¢„æµ‹

```python
import pandas as pd
import numpy as np
from tensorflow import keras
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

class StockPredictor:
    """è‚¡ç¥¨ä»·æ ¼é¢„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self, lookback=60):
        self.lookback = lookback
        self.scaler = MinMaxScaler()
        self.model = None
    
    def prepare_data(self, data):
        """å‡†å¤‡æ•°æ®"""
        # å½’ä¸€åŒ–
        scaled_data = self.scaler.fit_transform(data.reshape(-1, 1))
        
        X, y = [], []
        for i in range(self.lookback, len(scaled_data)):
            X.append(scaled_data[i-self.lookback:i, 0])
            y.append(scaled_data[i, 0])
        
        return np.array(X), np.array(y)
    
    def build_model(self):
        """æ„å»ºLSTMæ¨¡å‹"""
        self.model = keras.Sequential([
            layers.LSTM(50, return_sequences=True, 
                       input_shape=(self.lookback, 1)),
            layers.Dropout(0.2),
            layers.LSTM(50, return_sequences=True),
            layers.Dropout(0.2),
            layers.LSTM(50),
            layers.Dropout(0.2),
            layers.Dense(1)
        ])
        
        self.model.compile(optimizer='adam', loss='mse')
    
    def train(self, X_train, y_train, epochs=50, batch_size=32):
        """è®­ç»ƒæ¨¡å‹"""
        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
        
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.1,
            verbose=1
        )
        
        return history
    
    def predict(self, X_test):
        """é¢„æµ‹"""
        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
        predictions = self.model.predict(X_test)
        predictions = self.scaler.inverse_transform(predictions)
        return predictions
    
    def plot_results(self, y_true, y_pred):
        """å¯è§†åŒ–ç»“æœ"""
        plt.figure(figsize=(14, 6))
        plt.plot(y_true, label='å®é™…ä»·æ ¼', color='blue')
        plt.plot(y_pred, label='é¢„æµ‹ä»·æ ¼', color='red')
        plt.xlabel('æ—¶é—´')
        plt.ylabel('è‚¡ä»·')
        plt.title('è‚¡ç¥¨ä»·æ ¼é¢„æµ‹')
        plt.legend()
        plt.grid(True)
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
# å‡è®¾æœ‰è‚¡ç¥¨æ•°æ®
stock_data = np.random.randn(1000) * 10 + 100

# åˆ›å»ºé¢„æµ‹å™¨
predictor = StockPredictor(lookback=60)

# å‡†å¤‡æ•°æ®
X, y = predictor.prepare_data(stock_data)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
predictor.build_model()
predictor.train(X_train, y_train, epochs=50)

# é¢„æµ‹
predictions = predictor.predict(X_test)
y_test_inv = predictor.scaler.inverse_transform(y_test.reshape(-1, 1))

# å¯è§†åŒ–
predictor.plot_results(y_test_inv, predictions)
```

## 7.5 Bi-LSTMåŒå‘ç½‘ç»œ

```python
# åŒå‘LSTM
model = keras.Sequential([
    layers.Bidirectional(
        layers.LSTM(64, return_sequences=True),
        input_shape=(None, 10)
    ),
    layers.Bidirectional(layers.LSTM(32)),
    layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')
```

## 7.6 æ³¨æ„åŠ›æœºåˆ¶

```python
class Attention(layers.Layer):
    """æ³¨æ„åŠ›å±‚"""
    
    def __init__(self, units):
        super(Attention, self).__init__()
        self.W1 = layers.Dense(units)
        self.W2 = layers.Dense(units)
        self.V = layers.Dense(1)
    
    def call(self, query, values):
        # query shape: (batch_size, hidden_size)
        # values shape: (batch_size, seq_len, hidden_size)
        
        # æ‰©å±•ç»´åº¦
        query_with_time = tf.expand_dims(query, 1)
        
        # è®¡ç®—åˆ†æ•°
        score = self.V(tf.nn.tanh(
            self.W1(query_with_time) + self.W2(values)
        ))
        
        # æ³¨æ„åŠ›æƒé‡
        attention_weights = tf.nn.softmax(score, axis=1)
        
        # åŠ æƒæ±‚å’Œ
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        
        return context_vector, attention_weights
```

## ğŸ“š æœ¬ç« å°ç»“
- âœ… RNNåŸºæœ¬åŸç†å’Œå®ç°
- âœ… LSTMè§£å†³é•¿æœŸä¾èµ–é—®é¢˜
- âœ… GRUç®€åŒ–ç‰ˆLSTM
- âœ… è‚¡ç¥¨é¢„æµ‹å®æˆ˜é¡¹ç›®
- âœ… åŒå‘LSTMå’Œæ³¨æ„åŠ›æœºåˆ¶

[â¬…ï¸ ä¸Šä¸€ç« ](./06-å·ç§¯ç¥ç»ç½‘ç»œ.md) | [è¿”å›ç›®å½•](../README.md) | [ä¸‹ä¸€ç«  â¡ï¸](./08-Transformeræ¶æ„.md)
