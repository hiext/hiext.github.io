# ç¬¬8ç« ï¼šTransformeræ¶æ„

## ğŸ“ æœ¬ç« ç›®æ ‡
- ç†è§£æ³¨æ„åŠ›æœºåˆ¶åŸç†
- æŒæ¡Transformeræ¶æ„
- å­¦ä¹ BERTå’ŒGPTæ¨¡å‹
- å®ç°æ–‡æœ¬åˆ†ç±»ä»»åŠ¡

## 8.1 æ³¨æ„åŠ›æœºåˆ¶

```python
import torch
import torch.nn as nn

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super().__init__()
        self.d_k = d_k
    
    def forward(self, Q, K, V, mask=None):
        # Q, K, V: (batch_size, seq_len, d_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
```

## 8.2 Multi-Head Attention

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # Linear transformations
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn = torch.softmax(scores, dim=-1)
        context = torch.matmul(attn, V)
        
        # Concatenate heads
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.W_o(context)
        
        return output
```

## 8.3 Transformerå®Œæ•´å®ç°

```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # Multi-head attention
        attn_output = self.attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed-forward
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))
        
        return x
```

## 8.4 å®æˆ˜ï¼šä½¿ç”¨BERTè¿›è¡Œæ–‡æœ¬åˆ†ç±»

```python
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch

class TextClassifier:
    def __init__(self, model_name='bert-base-chinese', num_labels=2):
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForSequenceClassification.from_pretrained(
            model_name, num_labels=num_labels
        )
    
    def prepare_data(self, texts, labels):
        encodings = self.tokenizer(texts, truncation=True, padding=True, max_length=128)
        
        class Dataset(torch.utils.data.Dataset):
            def __init__(self, encodings, labels):
                self.encodings = encodings
                self.labels = labels
            
            def __getitem__(self, idx):
                item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
                item['labels'] = torch.tensor(self.labels[idx])
                return item
            
            def __len__(self):
                return len(self.labels)
        
        return Dataset(encodings, labels)
    
    def train(self, train_dataset, eval_dataset, epochs=3):
        training_args = TrainingArguments(
            output_dir='./results',
            num_train_epochs=epochs,
            per_device_train_batch_size=16,
            per_device_eval_batch_size=64,
            warmup_steps=500,
            weight_decay=0.01,
            logging_dir='./logs',
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset
        )
        
        trainer.train()
    
    def predict(self, texts):
        inputs = self.tokenizer(texts, return_tensors="pt", truncation=True, padding=True)
        outputs = self.model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=-1)
        return predictions.tolist()

# ä½¿ç”¨ç¤ºä¾‹
texts_train = ["è¿™ä¸ªäº§å“å¾ˆå¥½", "è´¨é‡å¤ªå·®äº†"]
labels_train = [1, 0]

classifier = TextClassifier()
train_dataset = classifier.prepare_data(texts_train, labels_train)
classifier.train(train_dataset, train_dataset)

predictions = classifier.predict(["è¿™ä¸ªå¾ˆä¸é”™"])
print(predictions)
```

## ğŸ“š æœ¬ç« å°ç»“
- âœ… æ³¨æ„åŠ›æœºåˆ¶åŸç†
- âœ… Transformerå®Œæ•´æ¶æ„
- âœ… BERTæ¨¡å‹åº”ç”¨
- âœ… æ–‡æœ¬åˆ†ç±»å®æˆ˜

[â¬…ï¸ ä¸Šä¸€ç« ](./07-å¾ªç¯ç¥ç»ç½‘ç»œ.md) | [è¿”å›ç›®å½•](../README.md) | [ä¸‹ä¸€ç«  â¡ï¸](./09-è®¡ç®—æœºè§†è§‰.md)
