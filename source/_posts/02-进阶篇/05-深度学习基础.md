# ç¬¬5ç« ï¼šæ·±åº¦å­¦ä¹ åŸºç¡€

## ğŸ“ æœ¬ç« ç›®æ ‡
- ç†è§£ç¥ç»ç½‘ç»œåŸç†
- æŒæ¡åå‘ä¼ æ’­ç®—æ³•
- å­¦ä¹ å¸¸è§æ¿€æ´»å‡½æ•°
- å®ç°æ‰‹å†™æ•°å­—è¯†åˆ«

## 5.1 ç¥ç»ç½‘ç»œåŸºç¡€

### 5.1.1 æ„ŸçŸ¥æœºæ¨¡å‹

```python
import numpy as np

class Perceptron:
    def __init__(self, n_inputs):
        self.weights = np.random.randn(n_inputs)
        self.bias = 0
    
    def predict(self, x):
        z = np.dot(x, self.weights) + self.bias
        return 1 if z > 0 else 0
    
    def train(self, X, y, epochs=100, lr=0.01):
        for _ in range(epochs):
            for xi, yi in zip(X, y):
                prediction = self.predict(xi)
                error = yi - prediction
                self.weights += lr * error * xi
                self.bias += lr * error
```

### 5.1.2 å¤šå±‚æ„ŸçŸ¥æœº(MLP)

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

class NeuralNetwork:
    def __init__(self, layers):
        self.weights = []
        self.biases = []
        
        for i in range(len(layers) - 1):
            w = np.random.randn(layers[i], layers[i+1])
            b = np.zeros((1, layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def forward(self, X):
        self.activations = [X]
        
        for w, b in zip(self.weights, self.biases):
            z = np.dot(self.activations[-1], w) + b
            a = sigmoid(z)
            self.activations.append(a)
        
        return self.activations[-1]
    
    def backward(self, X, y, lr=0.01):
        m = X.shape[0]
        
        # è®¡ç®—è¾“å‡ºå±‚è¯¯å·®
        delta = self.activations[-1] - y
        
        # åå‘ä¼ æ’­
        for i in reversed(range(len(self.weights))):
            dw = np.dot(self.activations[i].T, delta) / m
            db = np.sum(delta, axis=0, keepdims=True) / m
            
            if i > 0:
                delta = np.dot(delta, self.weights[i].T) * \
                        sigmoid_derivative(self.activations[i])
            
            self.weights[i] -= lr * dw
            self.biases[i] -= lr * db
```

## 5.2 æ¿€æ´»å‡½æ•°

```python
import numpy as np

# Sigmoid
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# ReLU
def relu(x):
    return np.maximum(0, x)

# Tanh
def tanh(x):
    return np.tanh(x)

# Leaky ReLU
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)
```

## 5.3 æŸå¤±å‡½æ•°

```python
import numpy as np

# å‡æ–¹è¯¯å·®(MSE)
def mse_loss(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# äº¤å‰ç†µæŸå¤±
def cross_entropy_loss(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred + 1e-7))

# äºŒå…ƒäº¤å‰ç†µ
def binary_cross_entropy(y_true, y_pred):
    return -np.mean(y_true * np.log(y_pred) + 
                    (1 - y_true) * np.log(1 - y_pred))
```

## 5.4 å®æˆ˜ï¼šMNISTæ‰‹å†™æ•°å­—è¯†åˆ«

```python
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

# åŠ è½½æ•°æ®
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# æ•°æ®é¢„å¤„ç†
x_train = x_train.reshape(-1, 784) / 255.0
x_test = x_test.reshape(-1, 784) / 255.0
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

# æ„å»ºæ¨¡å‹
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# è®­ç»ƒæ¨¡å‹
history = model.fit(
    x_train, y_train,
    batch_size=128,
    epochs=10,
    validation_split=0.2
)

# è¯„ä¼°
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f'æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}')
```

## 5.5 ä¼˜åŒ–ç®—æ³•

```python
# SGD
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
    
    def update(self, params, grads):
        for param, grad in zip(params, grads):
            param -= self.lr * grad

# Adam
class Adam:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.m = None
        self.v = None
        self.t = 0
    
    def update(self, params, grads):
        if self.m is None:
            self.m = [np.zeros_like(p) for p in params]
            self.v = [np.zeros_like(p) for p in params]
        
        self.t += 1
        
        for i, (param, grad) in enumerate(zip(params, grads)):
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)
            
            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)
            
            param -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-7)
```

## ğŸ“š æœ¬ç« å°ç»“
- âœ… ç¥ç»ç½‘ç»œåŸºæœ¬åŸç†
- âœ… åå‘ä¼ æ’­ç®—æ³•
- âœ… æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°
- âœ… MNISTæ‰‹å†™æ•°å­—è¯†åˆ«å®æˆ˜
- âœ… ä¼˜åŒ–ç®—æ³•(SGDã€Adam)

[â¬…ï¸ ä¸Šä¸€ç« ](../01-å…¥é—¨ç¯‡/04-æ•°å­¦åŸºç¡€.md) | [è¿”å›ç›®å½•](../README.md) | [ä¸‹ä¸€ç«  â¡ï¸](./06-å·ç§¯ç¥ç»ç½‘ç»œ.md)
