# ç¬¬25ç« ï¼šçŸ­è§†é¢‘å†…å®¹ç†è§£ç³»ç»Ÿ

## ğŸ“ é¡¹ç›®æ¦‚è¿°
å¼€å‘çŸ­è§†é¢‘å†…å®¹ç†è§£å’Œæ¨èç³»ç»Ÿï¼ŒåŒ…å«è§†é¢‘åˆ†ç±»ã€å†…å®¹å®¡æ ¸ã€æ™ºèƒ½å‰ªè¾‘ç­‰åŠŸèƒ½ã€‚

## 25.1 è§†é¢‘åˆ†ç±»

```python
import cv2
from tensorflow import keras

class VideoClassifier:
    def __init__(self):
        self.model = keras.applications.ResNet50(weights='imagenet')
    
    def extract_frames(self, video_path, num_frames=10):
        """æå–å…³é”®å¸§"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_interval = total_frames // num_frames
        
        frames = []
        for i in range(num_frames):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
        
        cap.release()
        return frames
    
    def classify_video(self, video_path):
        frames = self.extract_frames(video_path)
        
        predictions = []
        for frame in frames:
            frame = cv2.resize(frame, (224, 224))
            frame = keras.applications.resnet50.preprocess_input(frame)
            pred = self.model.predict(np.expand_dims(frame, axis=0))
            predictions.append(pred)
        
        # èšåˆé¢„æµ‹ç»“æœ
        avg_pred = np.mean(predictions, axis=0)
        return avg_pred
```

## 25.2 å†…å®¹å®¡æ ¸

```python
class ContentModeration:
    def __init__(self):
        self.text_classifier = pipeline("text-classification")
        self.image_classifier = pipeline("image-classification")
    
    def moderate_video(self, video_path, audio_text):
        """è§†é¢‘å†…å®¹å®¡æ ¸"""
        results = {
            'text_safe': True,
            'image_safe': True,
            'issues': []
        }
        
        # æ–‡æœ¬å®¡æ ¸
        text_result = self.text_classifier(audio_text)
        if text_result[0]['label'] == 'NEGATIVE':
            results['text_safe'] = False
            results['issues'].append('æ–‡æœ¬åŒ…å«ä¸å½“å†…å®¹')
        
        # å›¾åƒå®¡æ ¸
        frames = self._extract_frames(video_path)
        for frame in frames:
            img_result = self.image_classifier(frame)
            if self._is_inappropriate(img_result):
                results['image_safe'] = False
                results['issues'].append('å›¾åƒåŒ…å«ä¸å½“å†…å®¹')
                break
        
        results['approved'] = results['text_safe'] and results['image_safe']
        return results
```

## 25.3 æ™ºèƒ½å‰ªè¾‘

```python
class IntelligentVideoEditor:
    def __init__(self):
        pass
    
    def detect_highlights(self, video_path):
        """æ£€æµ‹ç²¾å½©ç‰‡æ®µ"""
        cap = cv2.VideoCapture(video_path)
        
        highlights = []
        frame_count = 0
        motion_threshold = 5000
        
        ret, prev_frame = cap.read()
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
            
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            diff = cv2.absdiff(gray, prev_gray)
            motion_score = np.sum(diff)
            
            if motion_score > motion_threshold:
                timestamp = frame_count / cap.get(cv2.CAP_PROP_FPS)
                highlights.append({
                    'timestamp': timestamp,
                    'motion_score': motion_score
                })
            
            prev_gray = gray
            frame_count += 1
        
        cap.release()
        return highlights
    
    def auto_clip(self, video_path, target_duration=30):
        """è‡ªåŠ¨å‰ªè¾‘"""
        highlights = self.detect_highlights(video_path)
        
        # é€‰æ‹©æœ€ç²¾å½©çš„ç‰‡æ®µ
        top_highlights = sorted(
            highlights,
            key=lambda x: x['motion_score'],
            reverse=True
        )[:5]
        
        return top_highlights
```

## 25.4 è§†é¢‘æ¨è

```python
class VideoRecommender:
    def __init__(self):
        self.user_history = {}
        self.video_features = {}
    
    def extract_video_features(self, video_id, video_data):
        """æå–è§†é¢‘ç‰¹å¾"""
        features = {
            'category': video_data['category'],
            'tags': video_data['tags'],
            'duration': video_data['duration'],
            'view_count': video_data['view_count'],
            'like_rate': video_data['likes'] / (video_data['views'] + 1)
        }
        
        self.video_features[video_id] = features
        return features
    
    def recommend(self, user_id, n_items=10):
        """æ¨èè§†é¢‘"""
        user_watched = self.user_history.get(user_id, [])
        
        # åŸºäºå†…å®¹çš„æ¨è
        recommendations = []
        for video_id, features in self.video_features.items():
            if video_id not in user_watched:
                score = self._calculate_score(user_id, features)
                recommendations.append((video_id, score))
        
        # æ’åºå¹¶è¿”å›top-N
        recommendations.sort(key=lambda x: x[1], reverse=True)
        return recommendations[:n_items]
    
    def _calculate_score(self, user_id, video_features):
        # ç®€åŒ–çš„è¯„åˆ†é€»è¾‘
        score = video_features['view_count'] * 0.3 + \
                video_features['like_rate'] * 0.7
        return score
```

## ğŸ“š æœ¬ç« å°ç»“
- âœ… è§†é¢‘åˆ†ç±»ä¸æ ‡æ³¨
- âœ… å†…å®¹å®¡æ ¸ç³»ç»Ÿ
- âœ… æ™ºèƒ½å‰ªè¾‘åŠŸèƒ½
- âœ… è§†é¢‘æ¨èç®—æ³•

**ğŸ‰ æ­å–œï¼æ‚¨å·²å®Œæˆå…¨éƒ¨25ç« çš„å­¦ä¹ ï¼**

[â¬…ï¸ ä¸Šä¸€ç« ](./24-æ™ºèƒ½æ•™è‚²å¹³å°.md) | [è¿”å›ç›®å½•](../README.md)
